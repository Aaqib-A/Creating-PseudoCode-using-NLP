{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing importing Files\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading File Data\n",
    "\n",
    "fileData=[]\n",
    "with open(\"Output.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        x = re.sub(\"=\", \" = \", line)\n",
    "        x = re.sub(\"  \",\" \", x)   \n",
    "        fileData.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Shelf Data from \"Data Generator\"\n",
    "import shelve\n",
    "import os\n",
    "\n",
    "dest_file=\"NLP_Shelfv11//\" + \"Dict_Lists\" + \".shlf\"\n",
    "\n",
    "shelf = shelve.open(dest_file)\n",
    "\n",
    "dataType_number = shelf[\"dataType_number\"]\n",
    "dataType_char = shelf[\"dataType_char\"]\n",
    "dataType_list = shelf[\"dataType_list\"]\n",
    "dataType_bool = shelf[\"dataType_bool\"]\n",
    "dataType_class = shelf[\"dataType_class\"]\n",
    "dataType_void = shelf[\"dataType_void\"]\n",
    "dataType = shelf[\"dataType\"]\n",
    "\n",
    "variables_list = shelf[\"variables_list\"]\n",
    "func_name = shelf[\"func_name\"]\n",
    "\n",
    "scope_of_variable = shelf[\"scope_of_variable\"]\n",
    "\n",
    "operators = shelf[\"operators\"]\n",
    "\n",
    "command_declare = shelf[\"command_declare\"]\n",
    "command_print = shelf[\"command_print\"]\n",
    "command_input = shelf[\"command_input\"]\n",
    "\n",
    "func_prog = shelf[\"func_prog\"]\n",
    "\n",
    "special_class = shelf[\"special_class\"]\n",
    "special_function = shelf[\"special_function\"]\n",
    "\n",
    "other_tags = shelf[\"other_tags\"]\n",
    "seperators = shelf[\"seperators\"] \n",
    "equal_symbol = shelf[\"equal_symbol\"]\n",
    "\n",
    "\n",
    "shelf.close()\n",
    "\n",
    "#print(dataType_number, \"\\n\")\n",
    "#print(dataType, \"\\n\")\n",
    "#print(variables_list, \"\\n\")\n",
    "#print(scope_of_variable, \"\\n\")\n",
    "#print(operators, \"\\n\")\n",
    "#print(command_declare, \"\\n\")\n",
    "#print(command_print, \"\\n\")\n",
    "#print(command_input, \"\\n\")\n",
    "#print(func_prog, \"\\n\")\n",
    "#print(special_class, \"\\n\")\n",
    "#print(special_function, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Patterns\n",
    "#{\"IN\":[\"\",\"s\"]}\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1_1 = [{\"LOWER\":{\"IN\":dataType}}]\n",
    "matcher.add(\"DATATYPE\", None, pattern1_1)\n",
    "\n",
    "pattern2_1 = [{\"LOWER\":\"global\"}]\n",
    "pattern2_2 = [{\"LOWER\":\"local\"}]\n",
    "matcher.add(\"SCOPE\", None, pattern2_1, pattern2_2)\n",
    "\n",
    "pattern3_1 = [{\"LOWER\":{\"IN\":operators}}]\n",
    "pattern3_2 = [{\"LOWER\":{\"IN\":[\"additions\", \"adds\"]}}]\n",
    "pattern3_3 = [{\"LOWER\":{\"IN\":[\"subtractions\", \"subs\"]}}]\n",
    "pattern3_4 = [{\"LOWER\":{\"IN\":[\"minuses\", \"multiplies\", \"muls\"]}}]\n",
    "pattern3_5 = [{\"LOWER\":{\"IN\":[\"divides\", \"divs\"]}}]\n",
    "pattern3_6 = [{\"LOWER\":{\"IN\":[\"modulus\", \"moduluses\", \"mods\"]}}]\n",
    "matcher.add(\"OPERATOR\", None, pattern3_1, pattern3_2, pattern3_3, pattern3_4, pattern3_5, pattern3_6)\n",
    "\n",
    "pattern4_1 = [{\"LOWER\":{\"IN\":command_declare}}]\n",
    "pattern4_2 = [{\"LOWER\":{\"IN\":command_print}}]\n",
    "pattern4_3 = [{\"LOWER\":{\"IN\":command_input}}]\n",
    "pattern4_4 = [{\"LOWER\":{\"IN\":special_function}}]\n",
    "matcher.add(\"COMMAND\", None, pattern4_1, pattern4_2, pattern4_3, pattern4_4)\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":[\"function\",\"functions\"]}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"func\",\"funcs\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"program\",\"programs\"]}}]\n",
    "pattern5_4 = [{\"LOWER\":{\"IN\":[\"programme\",\"programmes\"]}}]\n",
    "pattern5_5 = [{\"LOWER\":{\"IN\":[\"prog\",\"progs\"]}}]\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":func_prog}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"funcs\",\"functions\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"programs\",\"programmes\",\"progs\"]}}]\n",
    "matcher.add(\"FUNC_PROG\", None, pattern5_1, pattern5_2, pattern5_3)\n",
    "\n",
    "pattern6_1 = [{\"LOWER\": {\"IN\": variables_list}}]\n",
    "matcher.add(\"VARI\", None, pattern6_1)\n",
    "\n",
    "pattern7_1 = [{\"LOWER\": {\"IN\": special_class}}]\n",
    "matcher.add(\"SPL_CLASS\", None, pattern7_1)\n",
    "\n",
    "pattern8_1 = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"VALUE\", None, pattern8_1)\n",
    "\n",
    "pattern9_1 = [{\"LOWER\": {\"IN\": seperators}}]\n",
    "matcher.add(\"SEPERATOR\", None, pattern9_1)\n",
    "\n",
    "pattern10_1 = [{\"LOWER\":{\"IN\":equal_symbol}}]\n",
    "matcher.add(\"EQUAL\", None, pattern10_1)\n",
    "\n",
    "pattern11_1 = [{\"ORTH\": \"'\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"'\"}]\n",
    "pattern11_2 = [{\"ORTH\": \"\\\"\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"\\\"\"}]\n",
    "matcher.add(\"TEXT\", None, pattern11_1, pattern11_2)\n",
    "\n",
    "#pattern12_1 = [{\"ORTH\": \"two\"}]\n",
    "#pattern12_2 = [{\"ORTH\": \"3\"}]\n",
    "#matcher.add(\"COUNT\", None, pattern12_1, pattern12_2)\n",
    "\n",
    "#pattern12_1 = [{\"LOWER\": {\"IN\": other_tags}}]\n",
    "#matcher.add(\"OTHER\", None, pattern12_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Training Data\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(fileData):\n",
    "    \n",
    "    #x = re.sub(\"=\", \" = \", doc)\n",
    "    #doc = re.sub(\"  \",\" \", x)\n",
    "    \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    #print(spans)\n",
    "    \n",
    "    #Modified Start\n",
    "    entity_label=[]\n",
    "    entity_label = [(nlp.vocab.strings[match_id]) for match_id, start, end in matcher(doc)]\n",
    "    #print(entity_label)\n",
    "    \n",
    "    #Remove duplicates\n",
    "    #if len(spans) != len(set(spans))\n",
    "    \n",
    "\n",
    "    entities = [(span.start_char, span.end_char, entity_label[index]) for index,span in enumerate(spans)]\n",
    "    #print(entities)\n",
    "    #tags = [(entity_label[index]) for index,span in enumerate(spans)]\n",
    "    \n",
    "    #for eachWord in entities:\n",
    "    #    print(\"Entities: {}\".format(eachWord))\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    #training_example = (doc.text, {\"entities\": entities}, {\"tags\": tags})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    #Modified End\n",
    "    \n",
    "    '''\n",
    "    entities = [(span.start_char, span.end_char, \"INITIALIZATION\") for span in spans]\n",
    "    \n",
    "    for eachWord in entities:\n",
    "        print(\"Entities: {}\".format(eachWord))\n",
    "        \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    '''\n",
    "    \n",
    "\n",
    "#print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store to Data file for inspection\n",
    "with open(\"training_data.txt\",\"w\") as o_file:\n",
    "    for item in TRAINING_DATA:\n",
    "        o_file.write(\"{} \\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Pickle Data\n",
    "import pickle\n",
    "\n",
    "with open('training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(TRAINING_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pickle data\n",
    "with open('training_data.pkl', 'rb') as f:\n",
    "    TRAINING_DATA = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(37, 42, 'EQUAL')' and '(37, 45, 'EQUAL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-6666173f0c37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# Update the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m#annotations_tag = [tags for text, entities, tags in batch]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;31m# Allow dict of args to GoldParse, instead of GoldParse objects.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_docs_and_golds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_format_docs_and_golds\u001b[1;34m(self, docs, golds)\u001b[0m\n\u001b[0;32m    480\u001b[0m                     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE151\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munexpected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                 \u001b[0mgold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             \u001b[0mdoc_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mgold_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mgold.pyx\u001b[0m in \u001b[0;36mspacy.gold.GoldParse.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mgold.pyx\u001b[0m in \u001b[0;36mspacy.gold.biluo_tags_from_offsets\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(37, 42, 'EQUAL')' and '(37, 45, 'EQUAL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "#tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(tagger) \n",
    "\n",
    "\n",
    "ner.add_label(\"DATATYPE\")\n",
    "ner.add_label(\"SCOPE\")\n",
    "ner.add_label(\"OPERATOR\")\n",
    "ner.add_label(\"COMMAND\")\n",
    "ner.add_label(\"FUNC_PROG\")\n",
    "ner.add_label(\"VARI\")\n",
    "ner.add_label(\"SPL_CLASS\")\n",
    "ner.add_label(\"VALUE\")\n",
    "ner.add_label(\"COUNT\")\n",
    "ner.add_label(\"SEPERATOR\")\n",
    "ner.add_label(\"EQUAL\")\n",
    "ner.add_label(\"TEXT\")\n",
    "#ner.add_label(\"OTHER\")\n",
    "\n",
    "'''\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "tagger.add_label(\"COUNT\")\n",
    "tagger.add_label(\"SEPERATOR\")\n",
    "tagger.add_label(\"EQUAL\")\n",
    "tagger.add_label(\"TEXT\")\n",
    "tagger.add_label(\"OTHER\")\n",
    "'''\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(3):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1000):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        #for text, entities, tags in batch:\n",
    "        #    annotations.append(entities)   \n",
    "        #    annotations.append(tags)\n",
    "        \n",
    "        #print(texts)\n",
    "        #print(annotations)\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "               \n",
    "        #annotations_tag = [tags for text, entities, tags in batch]\n",
    "        #print(annotations_tag)\n",
    "        #nlp.update(texts, annotations_tag, losses=losses)\n",
    "        \n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display 0 7 COMMAND\n",
      "variable 8 16 DATATYPE\n",
      "text1 17 22 VARI\n",
      "equals 23 29 VARI\n",
      "'some thing something hello there. General Kenobi' 33 83 TEXT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    display\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COMMAND</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    variable\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATATYPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    text1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">VARI</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    equals\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">VARI</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    'some thing something hello there. General Kenobi'\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TEXT</span>\n",
       "</mark>\n",
       " on screen</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NER Test\n",
    "from spacy import displacy\n",
    "\n",
    "#doc = nlp(\"create a function to add numbers z and b\")\n",
    "#doc = nlp(\"if a > b then add a and b\")\n",
    "#doc = nlp(\"get input k\")\n",
    "#doc = nlp(\"if a>b then print a. If b>c then print c\")\n",
    "\n",
    "doc = (\"display variable text1 equals to 'some thing something hello there. General Kenobi' on screen\")\n",
    "#doc = re.sub(\"=\", \" = \", doc)\n",
    "#doc = re.sub(\"  \",\" \", doc) \n",
    "\n",
    "doc = nlp(doc)\n",
    "  \n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('Write', '\"\"', 'PUNCT'), ('a', '\"\"', 'PUNCT'), ('program', '\"\"', 'PUNCT'), ('for', '\"\"', 'PUNCT'), ('mod', '\"\"', 'PUNCT'), ('doubles', '\"\"', 'PUNCT'), ('b', '\"\"', 'PUNCT'), ('=', '\"\"', 'PUNCT'), ('77', '\"\"', 'PUNCT'), ('and', '\"\"', 'PUNCT'), ('c', '\"\"', 'PUNCT'), ('=', '\"\"', 'PUNCT'), ('doubles', '\"\"', 'PUNCT'), ('77', '\"\"', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Tagger Test\n",
    "\n",
    "test_text = \"Write a program for mod doubles b = 77 and c = doubles 77\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the nlp Object\n",
    "nlp.to_disk(\"./NLP_Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the nlp Object\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"./NLP_Object\")\n",
    "\n",
    "doc = nlp(\"create a function to add numbers a and b\")\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
