{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3a4c398b12b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfileData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Output.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Output.txt'"
     ]
    }
   ],
   "source": [
    "fileData=[]\n",
    "with open(\"Output.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        x = re.sub(\"=\", \" = \", line)\n",
    "        x = re.sub(\"  \",\" \", x)   \n",
    "        fileData.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import os\n",
    "\n",
    "dest_file=\"NLP_Shelfv8//\" + \"Dict_Lists\" + \".shlf\"\n",
    "\n",
    "shelf = shelve.open(dest_file)\n",
    "\n",
    "dataType_number = shelf[\"dataType_number\"]\n",
    "dataType_char = shelf[\"dataType_char\"]\n",
    "dataType_list = shelf[\"dataType_list\"]\n",
    "dataType_bool = shelf[\"dataType_bool\"]\n",
    "dataType_class = shelf[\"dataType_class\"]\n",
    "dataType_void = shelf[\"dataType_void\"]\n",
    "dataType = shelf[\"dataType\"]\n",
    "\n",
    "variables_list = shelf[\"variables_list\"]\n",
    "\n",
    "scope_of_variable = shelf[\"scope_of_variable\"]\n",
    "\n",
    "operators = shelf[\"operators\"]\n",
    "\n",
    "command_declare = shelf[\"command_declare\"]\n",
    "command_print = shelf[\"command_print\"]\n",
    "command_input = shelf[\"command_input\"]\n",
    "\n",
    "func_prog = shelf[\"func_prog\"]\n",
    "\n",
    "special_class = shelf[\"special_class\"]\n",
    "special_function = shelf[\"special_function\"]\n",
    "\n",
    "shelf.close()\n",
    "\n",
    "func_prog_start = [\n",
    "                 \"Write a program to\", \"Create a program to\", \"Write a program for\", \"Create a program for\",\n",
    "                 \"Write a function to\", \"Create a function to\",\"Write a function for\", \"Create a function for\",\n",
    "    \n",
    "                 \"Write program to\",  \"Create program for\",\n",
    "                 \"Write function to\", \"Create function for\",\n",
    "                 \"\"\n",
    "                ]\n",
    "\n",
    "other_tags = [\"a\", 'to', 'for']\n",
    "\n",
    "seperators = [\"and\", \",\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Patterns\n",
    "#{\"IN\":[\"\",\"s\"]}\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1_1 = [{\"LOWER\":{\"IN\":dataType}}]\n",
    "matcher.add(\"DATATYPE\", None, pattern1_1)\n",
    "\n",
    "pattern2_1 = [{\"LOWER\":\"global\"}]\n",
    "pattern2_2 = [{\"LOWER\":\"local\"}]\n",
    "matcher.add(\"SCOPE\", None, pattern2_1, pattern2_2)\n",
    "\n",
    "pattern3_1 = [{\"LOWER\":{\"IN\":operators}}]\n",
    "pattern3_2 = [{\"LOWER\":{\"IN\":[\"additions\", \"adds\"]}}]\n",
    "pattern3_3 = [{\"LOWER\":{\"IN\":[\"subtractions\", \"subs\"]}}]\n",
    "pattern3_4 = [{\"LOWER\":{\"IN\":[\"minuses\", \"multiplies\", \"muls\"]}}]\n",
    "pattern3_5 = [{\"LOWER\":{\"IN\":[\"divides\", \"divs\"]}}]\n",
    "pattern3_6 = [{\"LOWER\":{\"IN\":[\"modulus\", \"moduluses\", \"mods\"]}}]\n",
    "matcher.add(\"OPERATOR\", None, pattern3_1, pattern3_2, pattern3_3, pattern3_4, pattern3_5, pattern3_6)\n",
    "\n",
    "pattern4_1 = [{\"LOWER\":{\"IN\":command_declare}}]\n",
    "pattern4_2 = [{\"LOWER\":{\"IN\":command_print}}]\n",
    "pattern4_3 = [{\"LOWER\":{\"IN\":command_input}}]\n",
    "pattern4_4 = [{\"LOWER\":{\"IN\":special_function}}]\n",
    "matcher.add(\"COMMAND\", None, pattern4_1, pattern4_2, pattern4_3, pattern4_4)\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":[\"function\",\"functions\"]}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"func\",\"funcs\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"program\",\"programs\"]}}]\n",
    "pattern5_4 = [{\"LOWER\":{\"IN\":[\"programme\",\"programmes\"]}}]\n",
    "pattern5_5 = [{\"LOWER\":{\"IN\":[\"prog\",\"progs\"]}}]\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":func_prog}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"funcs\",\"functions\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"programs\",\"programmes\",\"progs\"]}}]\n",
    "matcher.add(\"FUNC_PROG\", None, pattern5_1, pattern5_2, pattern5_3)\n",
    "\n",
    "pattern6_1 = [{\"LOWER\": {\"IN\": variables_list}}]\n",
    "matcher.add(\"VARI\", None, pattern6_1)\n",
    "\n",
    "pattern7_1 = [{\"LOWER\": {\"IN\": special_class}}]\n",
    "matcher.add(\"SPL_CLASS\", None, pattern7_1)\n",
    "\n",
    "pattern8_1 = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"VALUE\", None, pattern8_1)\n",
    "\n",
    "pattern9_1 = [{\"LOWER\": {\"IN\": seperators}}]\n",
    "matcher.add(\"SEPERATOR\", None, pattern9_1)\n",
    "\n",
    "pattern10_1 = [{\"LOWER\": {\"IN\": other_tags}}]\n",
    "matcher.add(\"OTHER\", None, pattern10_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create variables\n",
    "\n",
    "def generate_variable(dataFill, append_extra=False):\n",
    "    return_list = []\n",
    "    s_included = False\n",
    "    \n",
    "    for each_dataFill in dataFill:\n",
    "        if (s_included==False):\n",
    "                for i in range(0,3):\n",
    "                    return_list.append(each_dataFill+str(i))\n",
    "                    return_list.append(each_dataFill+\"_\"+str(i))\n",
    "                    s_included = True\n",
    "                '''\n",
    "                for i in range(10,13):\n",
    "                    return_list.append(each_dataFill+str(i))\n",
    "                    return_list.append(each_dataFill+\"_\"+str(i))\n",
    "                    s_included = True\n",
    "                '''\n",
    "                \n",
    "        else:\n",
    "            s_included = False\n",
    "            \n",
    "    if (append_extra==True):        \n",
    "        return_list.append(\"i\")\n",
    "        return_list.append(\"j\")\n",
    "        return_list.append(\"k\")\n",
    "        #return_list.append(\"a\")\n",
    "        return_list.append(\"b\")\n",
    "        return_list.append(\"c\")\n",
    "        \n",
    "    return (return_list)\n",
    "            \n",
    "#print (variables_list)\n",
    "#print(generate_variable(dataType))\n",
    "#print(generate_variable(dataType_char))\n",
    "#print(generate_variable(dataType_list, True))\n",
    "#print(generate_variable([\"var\"]))\n",
    "\n",
    "#variables_list = generate_variable(dataType, True)\n",
    "#print (variables_list)\n",
    "\n",
    "def generate_partial_data(txt_code, datafill_list, input_list):\n",
    "        \n",
    "    returnList = []\n",
    "    \n",
    "    for eachLine in input_list:\n",
    "        if txt_code in eachLine:\n",
    "            for eachData in datafill_list:\n",
    "                tmp_txt = eachLine.replace(txt_code, eachData)\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                returnList.append(tmp_txt)\n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "    '''\n",
    "    generate_variable()\n",
    "    for eachLine in input_txt:\n",
    "        if \"SCOPE_OFVAR\" in eachLine:\n",
    "            for each_scope in scope_of_variable:\n",
    "                tmp_txt = eachLine.replace(\"SCOPE_OFVAR\", each_scope)\n",
    "                output_list.append( tmp_txt )\n",
    "        else:\n",
    "            output_list.append(eachLine)\n",
    "    '''\n",
    "\n",
    "def generate_partial_data_DTYPE(txt_code, datafill_list, input_list):\n",
    "        \n",
    "    returnList = []\n",
    "    \n",
    "    for eachLine in input_list:\n",
    "        \n",
    "        if txt_code in eachLine:\n",
    "            for eachData in datafill_list:\n",
    "                temp_returnList = []\n",
    "                tmp_txt = eachLine.replace(txt_code, eachData)\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                temp_returnList.append(tmp_txt)\n",
    "                if \"VARI\" in eachLine:\n",
    "                    \n",
    "                    tmp_var_list = generate_variable([eachData],True)\n",
    "                    tmp_list = generate_partial_data_VARI(\"VARI\", tmp_var_list, temp_returnList)\n",
    "                    returnList = returnList + tmp_list\n",
    "                    \n",
    "                    #print (\"EachData : \"+eachData)\n",
    "                    #print (\"tmp_var_list: {}\".format(tmp_var_list))\n",
    "                    #print(\"returnList: {}\".format(returnList))\n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "                else:\n",
    "                    returnList = returnList + temp_returnList          \n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "\n",
    "def generate_partial_data_VARI(txt_code, datafill_list, input_list):      \n",
    "    returnList = []\n",
    "    for eachLine in input_list:\n",
    "        if txt_code in eachLine:\n",
    "            count=0\n",
    "            for eachData in datafill_list: \n",
    "                tmp_txt = eachLine\n",
    "                while (txt_code in tmp_txt):\n",
    "                    if count >= len(datafill_list):\n",
    "                        count=0\n",
    "                        #print(\"Length :\",len(datafill_list))\n",
    "                    tmp_txt = tmp_txt.replace(txt_code, datafill_list[count],1)\n",
    "                    count+=1\n",
    "                #next(datafill_list,\"var1\") \n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                returnList.append(tmp_txt)\n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "\n",
    "#temp_gen_data1=[\"CMD_DECL SCOPE_OFVAR DTYPE\"]\n",
    "#print(my_func (\"DTYPE\", dataType, temp_gen_data1))\n",
    "\n",
    "\n",
    "#Substitute and create dataset\n",
    "import random\n",
    "def generate_training_data(input_txt):\n",
    "    generated_data = []\n",
    "    \n",
    "    for eachLine in input_txt:\n",
    "        print (\"eachLine: \" + eachLine)\n",
    "        \n",
    "        temp_gen_data1=[eachLine]\n",
    "        temp_gen_data2= generate_partial_data_DTYPE (\"DTYPE_NUM\", dataType_number, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data_DTYPE (\"DTYPE_CHAR\", dataType_char, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data_DTYPE (\"DTYPE_ARRAY\", dataType_list, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"DTYPE_BOOL\", dataType_bool, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"DTYPE_CLASS\", dataType_class, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data_DTYPE (\"DTYPE_VOID\", dataType_void, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        #temp_gen_data2 = generate_partial_data (\"DTYPE\", dataType, temp_gen_data1)\n",
    "        temp_gen_data2 = generate_partial_data_DTYPE (\"DTYPE\", dataType, temp_gen_data1)\n",
    "        \n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"SCOPE_OFVAR\", scope_of_variable, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"OPER\", operators, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"CMD_DECL\", command_declare, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"CMD_PRNT\", command_print, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"CMD_INPUT\", command_input, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"SPL_CLASS\", special_class, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"SPL_FUNC\", special_function, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"FUNC_PROG_START\", func_prog_start, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"FUNC_PROG\", func_prog, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"OTHER\", other_tags, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"SEPERATOR\", seperators, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        variable_list = generate_variable (dataType, append_extra=True)\n",
    "        temp_gen_data2 = generate_partial_data_VARI(\"VARI\", variable_list, temp_gen_data1)\n",
    "        \n",
    "        generated_data = generated_data + temp_gen_data2\n",
    "    \n",
    "    temp_gen_data1=[]\n",
    "    for eachLine in generated_data:\n",
    "        if \"RAND_NUM\" in eachLine:\n",
    "            random_number = random.randint(0,100)\n",
    "            tmp_txt = eachLine.replace(\"RAND_NUM\", str(random_number))\n",
    "            temp_gen_data1.append(tmp_txt)\n",
    "        else:\n",
    "            temp_gen_data1.append(eachLine)\n",
    "    return (temp_gen_data1)\n",
    "   \n",
    "    \n",
    "#training_data = generate_training_data()\n",
    "\n",
    "#print (\"Length: \" + str( len(training_data) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eachLine: DTYPE\n",
      "eachLine: SCOPE_OFVAR\n",
      "eachLine: OPER\n",
      "eachLine: CMD_DECL\n",
      "eachLine: CMD_PRNT\n",
      "eachLine: CMD_INPUT\n",
      "eachLine: SPL_CLASS\n",
      "eachLine: SPL_FUNC\n",
      "eachLine: FUNC_PROG\n",
      "eachLine: VARI\n",
      "eachLine: RAND_NUM\n",
      "eachLine: SEPERATOR\n",
      "eachLine: OTHER\n",
      "Length: 230\n"
     ]
    }
   ],
   "source": [
    "#Create Output File - #Prepare Data\n",
    "\n",
    "#DTYPE_NUM    : dataType_number\n",
    "#DTYPE_CHAR   : dataType_char\n",
    "#DTYPE_ARRAY  :dataType_list\n",
    "#DTYPE_BOOL   :dataType_bool\n",
    "#DTYPE_CLASS  :dataType_class\n",
    "#DTYPE_VOID   :dataType_void\n",
    "#DTYPE : dataType\n",
    "\n",
    "#SCOPE_OFVAR : scope_of_variable\n",
    "\n",
    "#OPER : operators\n",
    "#\n",
    "#CMD_DECL  : command_declare\n",
    "#CMD_PRNT  : command_print\n",
    "#CMD_INPUT : command_input\n",
    "\n",
    "#SPL_CLASS: special_class   : [\"log\", \"printer\"]\n",
    "#SPL_FUNC: special_function : [\"save\"]\n",
    "\n",
    "#FUNC_PROG : func_prog\n",
    "#FUNC_PROG_START : func_prog_start\n",
    "\n",
    "#VARI : <Variables>\n",
    "#RAND_NUM : <Random Number>\n",
    "input_txt=[]\n",
    "\n",
    "\n",
    "input_txt=[\n",
    "        \"DTYPE\",\n",
    "        \"SCOPE_OFVAR\",\n",
    "        \"OPER\",\n",
    "        \"CMD_DECL\",\n",
    "        \"CMD_PRNT\",\n",
    "        \"CMD_INPUT\",\n",
    "        \"SPL_CLASS\",\n",
    "        \"SPL_FUNC\",\n",
    "        \"FUNC_PROG\",\n",
    "        \"VARI\",\n",
    "        \"RAND_NUM\",\n",
    "        \"SEPERATOR\",\n",
    "        \"OTHER\",\n",
    "          ]\n",
    "\n",
    "training_data = generate_training_data(input_txt)\n",
    "variables_list = generate_variable(dataType, True)\n",
    "\n",
    "print (\"Length: \" + str( len(training_data) ) )\n",
    "#print (training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n"
     ]
    }
   ],
   "source": [
    "#Creating Training Data\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(training_data):\n",
    "#for doc in nlp.pipe(fileData):\n",
    "    \n",
    "    #x = re.sub(\"=\", \" = \", doc)\n",
    "    #doc = re.sub(\"  \",\" \", x)\n",
    "    \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    \n",
    "    #Modified Start\n",
    "    entity_label=[]\n",
    "    entity_label = [(nlp.vocab.strings[match_id]) for match_id, start, end in matcher(doc)]\n",
    "    #if entity_label!=[]:\n",
    "    #    print (entity_label)\n",
    "    #entities = [(span.start_char, span.end_char, entity_label[index]) for index,span in enumerate(spans)]\n",
    "    entities = [entity_label[index] for index,span in enumerate(spans)]\n",
    "    \n",
    "    #for eachWord in entities:\n",
    "    #    print(\"Entities: {}\".format(eachWord))\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"tags\": entities})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    #Modified End\n",
    "    \n",
    "    '''\n",
    "    entities = [(span.start_char, span.end_char, \"INITIALIZATION\") for span in spans]\n",
    "    \n",
    "    for eachWord in entities:\n",
    "        print(\"Entities: {}\".format(eachWord))\n",
    "        \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    '''\n",
    "    \n",
    "\n",
    "#print(*TRAINING_DATA, sep=\"\\n\")\n",
    "print(len(TRAINING_DATA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store to Data file for inspection\n",
    "with open(\"training_data_pos.txt\",\"w\") as o_file:\n",
    "    for item in TRAINING_DATA:\n",
    "        o_file.write(\"{}\\n\".format(item))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('test_1.pkl', 'wb') as f:\n",
    "    pickle.dump(TRAINING_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tagger': 2.9545462131500244}\n",
      "{'tagger': 2.9261841773986816}\n",
      "{'tagger': 2.8574273586273193}\n",
      "{'tagger': 2.7157344818115234}\n",
      "{'tagger': 2.4473958015441895}\n",
      "{'tagger': 2.001467704772949}\n",
      "{'tagger': 1.4142560958862305}\n",
      "{'tagger': 0.8392866849899292}\n",
      "{'tagger': 0.426355242729187}\n",
      "{'tagger': 0.19063133001327515}\n",
      "{'tagger': 0.07766228169202805}\n",
      "{'tagger': 0.02974286675453186}\n",
      "{'tagger': 0.011045444756746292}\n",
      "{'tagger': 0.004034437704831362}\n",
      "{'tagger': 0.0014622414018958807}\n",
      "{'tagger': 0.000528862583450973}\n",
      "{'tagger': 0.0001931010774569586}\n",
      "{'tagger': 7.271709910128266e-05}\n",
      "{'tagger': 2.8667127480730414e-05}\n",
      "{'tagger': 1.193772641272517e-05}\n",
      "{'tagger': 5.272452654025983e-06}\n",
      "{'tagger': 2.47662410401972e-06}\n",
      "{'tagger': 1.2305206382734468e-06}\n",
      "{'tagger': 6.481674290625961e-07}\n",
      "{'tagger': 3.594076929402945e-07}\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "import random\n",
    "\n",
    "#nlp = spacy.blank(\"en\")\n",
    "#pos_tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(pos_tagger)\n",
    "    \n",
    "nlp = spacy.blank(\"en\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger) \n",
    "\n",
    "TRAINING_DATA = [['write a program',{'tags':['COMMAND', 'OTHER', 'FUNC_PROG']}]]\n",
    "#TRAINING_DATA=[]\n",
    "#with open('test_1.pkl', 'rb') as f:\n",
    "#    TRAINING_DATA = pickle.load(f)\n",
    "\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "tagger.add_label(\"OTHER\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "#if None in TRAINING_DATA:\n",
    "#    print(\"Hello there! General Error\")\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(25):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1000):\n",
    "\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        #texts, annotations = zip(batch)\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    '''\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1):\n",
    "\n",
    "        texts = [\"write a program\"]\n",
    "        annotations = [{'tags':['COMMAND', 'OTHER', 'FUNC_PROG']}]\n",
    "        \n",
    "        #texts, annotations = zip(batch)\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    '''\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('Write', 'COMMAND', 'X'), ('a', 'OTHER', 'X'), ('program', 'FUNC_PROG', 'X'), ('for', 'FUNC_PROG', 'X'), ('mod', 'OTHER', 'X'), ('doubles', 'COMMAND', 'X'), ('b', 'OTHER', 'X'), ('=', '\"\"', 'PUNCT'), ('77', 'COMMAND', 'X'), ('and', 'OTHER', 'X'), ('c', 'FUNC_PROG', 'X'), ('=', '\"\"', 'PUNCT'), ('doubles', 'FUNC_PROG', 'X'), ('77', 'FUNC_PROG', 'X')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Write a program for mod doubles b = 77 and c = doubles 77\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tagger': 0.0}\n",
      "Losses {'tagger': 0.0}\n",
      "Losses {'tagger': 0.0}\n",
      "Tags [('I', 'PRP', 'PRON'), ('like', 'VBP', 'VERB'), ('Afrotropical', 'NNP', 'PROPN'), ('apraxic', 'NN', 'NOUN'), ('blue', 'JJ', 'ADJ'), ('eggs', 'NNS', 'NOUN'), ('and', 'CC', 'CCONJ'), ('Afrocentricity', 'NNP', 'PROPN'), ('.', '.', 'PUNCT'), ('A', 'DT', 'DET'), ('Eurotransplant', 'NNP', 'PROPN'), ('is', 'VBZ', 'AUX'), ('cool', 'JJ', 'ADJ'), ('too', 'RB', 'ADV'), ('.', '.', 'PUNCT'), ('The', 'DT', 'DET'), ('agnathostomatous', 'JJ', 'ADJ'), ('Euromarket', 'NNP', 'PROPN'), ('and', 'CC', 'CCONJ'), ('asypnapsis', 'NNP', 'PROPN'), ('is', 'VBZ', 'AUX'), ('even', 'RB', 'ADV'), ('cooler', 'JJR', 'ADJ'), ('.', '.', 'PUNCT'), ('What', 'WP', 'PRON'), ('about', 'IN', 'ADP'), ('Eurocentrism', 'NN', 'NOUN'), ('?', '.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "TAG_MAP = {\"noun\": {\"pos\": \"NOUN\"}, \"verb\": {\"pos\": \"VERB\"}, \"adj\": {\"pos\": \"ADJ\"}, \"adv\": {\"pos\": \"ADV\"}}\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    ('Afrotropical', {'tags': ['adj']}), ('Afrocentricity', {'tags': ['noun']}),\n",
    "    ('Afrocentric', {'tags': ['adj']}), ('Afrocentrism', {'tags': ['noun']}),\n",
    "    ('Anglomania', {'tags': ['noun']}), ('Anglocentric', {'tags': ['adj']}),\n",
    "    ('apraxic', {'tags': ['adj']}), ('aglycosuric', {'tags': ['adj']}),\n",
    "    ('asecretory', {'tags': ['adj']}), ('aleukaemic', {'tags': ['adj']}),\n",
    "    ('agrin', {'tags': ['adj']}), ('Eurotransplant', {'tags': ['noun']}),\n",
    "    ('Euromarket', {'tags': ['noun']}), ('Eurocentrism', {'tags': ['noun']}),\n",
    "    ('adendritic', {'tags': ['adj']}), ('asynaptic', {'tags': ['adj']}),\n",
    "    ('Asynapsis', {'tags': ['noun']}), ('ametabolic', {'tags': ['adj']})\n",
    "]\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['ner', 'parser'])\n",
    "\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(3):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "# test the trained model\n",
    "test_text = \"I like Afrotropical apraxic blue eggs and Afrocentricity. A Eurotransplant is cool too. The agnathostomatous Euromarket and asypnapsis is even cooler. What about Eurocentrism?\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
