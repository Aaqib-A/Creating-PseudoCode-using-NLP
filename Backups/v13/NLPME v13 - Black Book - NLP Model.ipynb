{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing importing Files\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading File Data\n",
    "\n",
    "fileData=[]\n",
    "import os\n",
    "\n",
    "dest_folder = \"NLP_ModelData_v13\"\n",
    "input_file_loc = os.path.join(dest_folder, \"Dataset_DataGenerator.txt\")\n",
    "\n",
    "with open(input_file_loc, \"r\") as file:\n",
    "    for line in file:\n",
    "        x = re.sub(\"=\", \" = \", line)\n",
    "        x = re.sub(\"  \",\" \", x)   \n",
    "        fileData.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Shelf Data from \"Data Generator\"\n",
    "import shelve\n",
    "import os\n",
    "\n",
    "dest_file = os.path.join(dest_folder, \"Dict_Lists.shlf\")\n",
    "\n",
    "shelf = shelve.open(dest_file)\n",
    "\n",
    "dataType_number = shelf[\"dataType_number\"]\n",
    "dataType_char = shelf[\"dataType_char\"]\n",
    "dataType_list = shelf[\"dataType_list\"]\n",
    "dataType_bool = shelf[\"dataType_bool\"]\n",
    "dataType_class = shelf[\"dataType_class\"]\n",
    "dataType_void = shelf[\"dataType_void\"]\n",
    "dataType = shelf[\"dataType\"]\n",
    "\n",
    "variables_list = shelf[\"variables_list\"]\n",
    "func_name = shelf[\"func_name\"]\n",
    "\n",
    "scope_of_variable = shelf[\"scope_of_variable\"]\n",
    "\n",
    "operators = shelf[\"operators\"]\n",
    "\n",
    "command_declare = shelf[\"command_declare\"]\n",
    "command_print = shelf[\"command_print\"]\n",
    "command_input = shelf[\"command_input\"]\n",
    "\n",
    "func_prog = shelf[\"func_prog\"]\n",
    "\n",
    "special_class = shelf[\"special_class\"]\n",
    "special_function = shelf[\"special_function\"]\n",
    "\n",
    "other_tags = shelf[\"other_tags\"]\n",
    "seperators = shelf[\"seperators\"] \n",
    "equal_symbol = shelf[\"equal_symbol\"]\n",
    "\n",
    "\n",
    "shelf.close()\n",
    "\n",
    "#print(dataType_number, \"\\n\")\n",
    "#print(dataType, \"\\n\")\n",
    "#print(variables_list, \"\\n\")\n",
    "#print(scope_of_variable, \"\\n\")\n",
    "#print(operators, \"\\n\")\n",
    "#print(command_declare, \"\\n\")\n",
    "#print(command_print, \"\\n\")\n",
    "#print(command_input, \"\\n\")\n",
    "#print(func_prog, \"\\n\")\n",
    "#print(special_class, \"\\n\")\n",
    "#print(special_function, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Patterns\n",
    "#{\"IN\":[\"\",\"s\"]}\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1_1 = [{\"LOWER\":{\"IN\":dataType}}]\n",
    "matcher.add(\"DATATYPE\", None, pattern1_1)\n",
    "\n",
    "pattern2_1 = [{\"LOWER\":\"global\"}]\n",
    "pattern2_2 = [{\"LOWER\":\"local\"}]\n",
    "matcher.add(\"SCOPE\", None, pattern2_1, pattern2_2)\n",
    "\n",
    "pattern3_1 = [{\"LOWER\":{\"IN\":operators}}]\n",
    "pattern3_2 = [{\"LOWER\":{\"IN\":[\"additions\", \"adds\"]}}]\n",
    "pattern3_3 = [{\"LOWER\":{\"IN\":[\"subtractions\", \"subs\"]}}]\n",
    "pattern3_4 = [{\"LOWER\":{\"IN\":[\"minuses\", \"multiplies\", \"muls\"]}}]\n",
    "pattern3_5 = [{\"LOWER\":{\"IN\":[\"divides\", \"divs\"]}}]\n",
    "pattern3_6 = [{\"LOWER\":{\"IN\":[\"modulus\", \"moduluses\", \"mods\"]}}]\n",
    "matcher.add(\"OPERATOR\", None, pattern3_1, pattern3_2, pattern3_3, pattern3_4, pattern3_5, pattern3_6)\n",
    "\n",
    "pattern4_1 = [{\"LOWER\":{\"IN\":command_declare}}]\n",
    "matcher.add(\"INIT_COMMAND\", None, pattern4_1)\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":command_print}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":command_input}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":special_function}}]\n",
    "matcher.add(\"COMMAND\", None, pattern5_1, pattern5_2, pattern5_3)\n",
    "\n",
    "pattern6_1 = [{\"LOWER\":{\"IN\":func_prog}}]\n",
    "pattern6_2 = [{\"LOWER\":{\"IN\":[\"funcs\",\"functions\"]}}]\n",
    "pattern6_3 = [{\"LOWER\":{\"IN\":[\"programs\",\"programmes\",\"progs\", \"prgs\"]}}]\n",
    "matcher.add(\"FUNC_PROG\", None, pattern6_1, pattern6_2, pattern6_3)\n",
    "\n",
    "pattern7_1 = [{\"LOWER\": {\"IN\": variables_list}}]\n",
    "matcher.add(\"VARI\", None, pattern7_1)\n",
    "\n",
    "pattern8_1 = [{\"LOWER\": {\"IN\": special_class}}]\n",
    "matcher.add(\"SPL_CLASS\", None, pattern8_1)\n",
    "\n",
    "pattern9_1 = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"VALUE\", None, pattern9_1)\n",
    "\n",
    "pattern10_1 = [{\"LOWER\": {\"IN\": seperators}}]\n",
    "matcher.add(\"SEPERATOR\", None, pattern10_1)\n",
    "\n",
    "#pattern10_1 = [{\"LOWER\":{\"IN\":equal_symbol}}, {\"ORTH\": \"to\", \"OP\":\"?\"}]\n",
    "#pattern10_1 = [{\"TEXT\": {\"REGEX\": u\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"}}]\n",
    "#pattern10_2 = [{\"TEXT\": {\"REGEX\": \"[Ee][Qq][Uu][Aa][Ll][Ss]?\"}}, {\"ORTH\": {\"REGEX\": \" [Tt][Oo]\"}, \"OP\":\"?\" }]\n",
    "#pattern10_2 = [{\"LOWER\":\"equals\"}, {\"TEXT\": {\"REGEX\": \"( [Tt][Oo])\"}, \"OP\":\"?\"}]\n",
    "#pattern10_3 = [{\"LOWER\":\"equal\"}, {\"TEXT\": {\"REGEX\": \"( [Tt][Oo])\"}, \"OP\":\"?\"}]\n",
    "#matcher.add(\"EQUAL\", None, pattern10_1, pattern10_2)#, pattern10_3)#, pattern10_4, pattern10_5)\n",
    "pattern10_1 = [{\"ORTH\":\"=\"}]\n",
    "matcher.add(\"EQUAL\", None, pattern10_1)\n",
    "\n",
    "pattern11_1 = [{\"ORTH\": \"'\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"'\"}]\n",
    "pattern11_2 = [{\"ORTH\": \"\\\"\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"\\\"\"}]\n",
    "matcher.add(\"TEXT\", None, pattern11_1, pattern11_2)\n",
    "\n",
    "pattern12_1 = [{\"LOWER\": {\"IN\": func_name}}]\n",
    "matcher.add(\"FUNC_NAME\", None, pattern12_1)\n",
    "\n",
    "#pattern12_1 = [{\"ORTH\": \"two\"}]\n",
    "#pattern12_2 = [{\"ORTH\": \"3\"}]\n",
    "#matcher.add(\"COUNT\", None, pattern12_1, pattern12_2)\n",
    "\n",
    "#pattern12_1 = [{\"LOWER\": {\"IN\": other_tags}}]\n",
    "#matcher.add(\"OTHER\", None, pattern12_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Training Data\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(fileData):\n",
    "    \n",
    "    #x = re.sub(\"=\", \" = \", doc)\n",
    "    #doc = re.sub(\"  \",\" \", x)\n",
    "    \n",
    "    '''\n",
    "    #Regex Matcher - equals to\n",
    "    expression = r\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        span.append(doc.char_span(start, end))\n",
    "        entity.append(\"EQUAL\")\n",
    "        # This is a Span object or None if match doesn't map to valid token sequence\n",
    "        #if span is not None:\n",
    "        #    print(\"Found match:\", span.text, span.start_char, span.end_char)\n",
    "    '''\n",
    "     \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    #print(spans)\n",
    "    \n",
    "    #Modified Start\n",
    "    entity_label=[]\n",
    "    entity_label = [(nlp.vocab.strings[match_id]) for match_id, start, end in matcher(doc)]\n",
    "    #print(entity_label)\n",
    "    \n",
    "    #Regex Matcher - equals to\n",
    "    expression = r\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        start, end = match.span()\n",
    "        #span = doc.char_span(start, end)\n",
    "        spans.append(doc.char_span(start, end))\n",
    "        entity_label.append(\"EQUAL\")\n",
    "    \n",
    "\n",
    "    entities = [(span.start_char, span.end_char, entity_label[index]) for index,span in enumerate(spans)]\n",
    "    #print(entities)\n",
    "    #tags = [(entity_label[index]) for index,span in enumerate(spans)]\n",
    "    \n",
    "    #for eachWord in entities:\n",
    "    #    print(\"Entities: {}\".format(eachWord))\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    #training_example = (doc.text, {\"entities\": entities}, {\"tags\": tags})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    #Modified End\n",
    "    \n",
    "    '''\n",
    "    entities = [(span.start_char, span.end_char, \"INITIALIZATION\") for span in spans]\n",
    "    \n",
    "    for eachWord in entities:\n",
    "        print(\"Entities: {}\".format(eachWord))\n",
    "        \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    '''\n",
    "    \n",
    "\n",
    "#print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store to Data file for inspection\n",
    "\n",
    "train_data_loc = os.path.join(dest_folder, \"training_data.txt\")\n",
    "with open(train_data_loc,\"w\") as o_file:\n",
    "    for item in TRAINING_DATA:\n",
    "        o_file.write(\"{} \\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Pickle Data\n",
    "import pickle\n",
    "\n",
    "train_pickle_loc = os.path.join(dest_folder, \"training_data.pkl\")\n",
    "\n",
    "with open(train_pickle_loc, 'wb') as f:\n",
    "    pickle.dump(TRAINING_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pickle data\n",
    "with open(train_pickle_loc, 'rb') as f:\n",
    "    TRAINING_DATA = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n",
      "Optimizer: \t SGD\n",
      "Learn Rate: \t 0.001\n",
      "Beta1: \t\t 0.9\n",
      "Beta2: \t\t 0.999\n",
      "Epsilon: \t 1e-08\n",
      "L2: \t\t 1e-06\n",
      "Max Grad Norm: \t 1.0\n",
      "Batch Size: \t 5000\n",
      "Droupout Rate: \t 0.1\n",
      "Total Epoch: \t 5\n",
      "====================================\n",
      "Epoch:1/5\n",
      "Losses: {'ner': 611403.1619873047}\n",
      "Time Since Start: 99.1434006690979\n",
      "====================================\n",
      "Epoch:2/5\n",
      "Losses: {'ner': 200406.1436767578}\n",
      "Time Since Start: 199.16060137748718\n",
      "====================================\n",
      "Epoch:3/5\n",
      "Losses: {'ner': 9531.2309268713}\n",
      "Time Since Start: 300.4048020839691\n",
      "====================================\n",
      "Epoch:4/5\n",
      "Losses: {'ner': 481.8201700129721}\n",
      "Time Since Start: 405.4708025455475\n",
      "====================================\n",
      "Epoch:5/5\n",
      "Losses: {'ner': 7.4357797083963675}\n",
      "Time Since Start: 511.4210035800934\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "#tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(tagger) \n",
    "\n",
    "\n",
    "ner.add_label(\"DATATYPE\")\n",
    "ner.add_label(\"SCOPE\")\n",
    "ner.add_label(\"OPERATOR\")\n",
    "ner.add_label(\"INIT_COMMAND\")\n",
    "ner.add_label(\"COMMAND\")\n",
    "ner.add_label(\"FUNC_PROG\")\n",
    "ner.add_label(\"FUNC_NAME\")\n",
    "ner.add_label(\"VARI\")\n",
    "ner.add_label(\"SPL_CLASS\")\n",
    "ner.add_label(\"VALUE\")\n",
    "ner.add_label(\"COUNT\")\n",
    "ner.add_label(\"SEPERATOR\")\n",
    "ner.add_label(\"EQUAL\")\n",
    "ner.add_label(\"TEXT\")\n",
    "#ner.add_label(\"OTHER\")\n",
    "\n",
    "'''\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"FUNC_NAME\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "tagger.add_label(\"COUNT\")\n",
    "tagger.add_label(\"SEPERATOR\")\n",
    "tagger.add_label(\"EQUAL\")\n",
    "tagger.add_label(\"TEXT\")\n",
    "tagger.add_label(\"OTHER\")\n",
    "'''\n",
    "\n",
    "'''\n",
    "optimizer = SGD(\n",
    "    learn_rate=0.001,\n",
    "    L2=1e-6,\n",
    "    grad_clip=1.0\n",
    ")\n",
    "'''\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "\n",
    "optimizer = ner.create_optimizer()\n",
    "optimizer.learn_rate = 0.001\n",
    "optimizer.beta1=0.9\n",
    "optimizer.beta2=0.999\n",
    "optimizer.eps=1e-8\n",
    "optimizer.L2=1e-6\n",
    "optimizer.max_grad_norm=1.0\n",
    "batch_size = 5000\n",
    "dropout_rate = 0.1\n",
    "total_epoch=5\n",
    "\n",
    "#Printing Start\n",
    "print(\"Optimizer: \\t\", \"SGD\")\n",
    "print(\"Learn Rate: \\t\", optimizer.learn_rate)\n",
    "print(\"Beta1: \\t\\t\", optimizer.beta1)\n",
    "print(\"Beta2: \\t\\t\", optimizer.beta2)\n",
    "print(\"Epsilon: \\t\", optimizer.eps)\n",
    "print(\"L2: \\t\\t\", optimizer.L2)\n",
    "print(\"Max Grad Norm: \\t\", optimizer.max_grad_norm)\n",
    "print(\"Batch Size: \\t\", batch_size)\n",
    "print(\"Droupout Rate: \\t\", dropout_rate)\n",
    "print(\"Total Epoch: \\t\", total_epoch)\n",
    "\n",
    "#Printing End\n",
    "\n",
    "for itn in range(total_epoch):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=batch_size):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        #for text, entities, tags in batch:\n",
    "        #    annotations.append(entities)   \n",
    "        #    annotations.append(tags)\n",
    "        \n",
    "        #print(texts)\n",
    "        #print(annotations)\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses, drop=dropout_rate, sgd=optimizer)\n",
    "               \n",
    "        #annotations_tag = [tags for text, entities, tags in batch]\n",
    "        #print(annotations_tag)\n",
    "        #nlp.update(texts, annotations_tag, losses=losses)\n",
    "    print(\"====================================\")    \n",
    "    print(\"Epoch:{}/{}\".format(itn+1, total_epoch))\n",
    "    print(\"Losses: \"+ str(losses))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Time Since Start:\", end_time - start_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the nlp Object\n",
    "nlp.to_disk(\"./NLP_Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'ents_p': 100.0, 'ents_r': 100.0, 'ents_f': 100.0, 'ents_per_type': {'COMMAND': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'SCOPE': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'INIT_COMMAND': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'DATATYPE': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'FUNC_PROG': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'TEXT': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'EQUAL': {'p': 100.0, 'r': 100.0, 'f': 100.0}, 'VARI': {'p': 100.0, 'r': 100.0, 'f': 100.0}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "#Testing the models\n",
    "#https://stackoverflow.com/questions/44827930/evaluation-in-a-spacy-ner-model\n",
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "# example run\n",
    "\n",
    "examples = [\n",
    "    (\"Write a function to creates global integer\",\n",
    "     [(0, 5, 'COMMAND'), (8, 16, 'FUNC_PROG'), (20, 27, 'INIT_COMMAND'), (28, 34, 'SCOPE'), (35, 42, 'DATATYPE')]),\n",
    "    (\"Write function to sets global strs strs1 = 'please enter value'\",\n",
    "     [(0, 5, 'COMMAND'), (6, 14, 'FUNC_PROG'), (18, 22, 'INIT_COMMAND'), (23, 29, 'SCOPE'), (30, 34, 'DATATYPE'), (35, 40, 'VARI'), (41, 42, 'EQUAL'), (43, 63, 'TEXT')])\n",
    "]\n",
    "\n",
    "ner_model = spacy.load(\"./NLP_Object\") # for spaCy's pretrained use 'en_core_web_sm'\n",
    "results = evaluate(ner_model, examples)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 0 3 COMMAND\n",
      "input 4 9 COMMAND\n",
      "k 10 11 VARI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    get\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COMMAND</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    input\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COMMAND</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    k\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">VARI</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NER Test\n",
    "from spacy import displacy\n",
    "\n",
    "#doc = (\"create a function to add numbers z and b\")\n",
    "#doc = (\"if a > b then add a and b\")\n",
    "doc = (\"get input k\")\n",
    "#doc = (\"if a>b then print a. If b>c then print c\")\n",
    "\n",
    "#doc = (\"display variable text1 equals to 'Wubba lubba dubdub! General Kenobi A sentence' on screen\")\n",
    "#doc = (\"Create a program and name it wazza_wazza for create local integer equal to 45\")\n",
    "\n",
    "#doc = re.sub(\"=\", \" = \", doc)\n",
    "#doc = re.sub(\"  \",\" \", doc) \n",
    "\n",
    "doc = nlp(doc)\n",
    "  \n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('Write', '', ''), ('a', '', ''), ('program', '', ''), ('for', '', ''), ('mod', '', ''), ('doubles', '', ''), ('b', '', ''), ('=', '', ''), ('77', '', ''), ('and', '', ''), ('c', '', ''), ('=', '', ''), ('doubles', '', ''), ('77', '', '')]\n"
     ]
    }
   ],
   "source": [
    "# Tagger Test\n",
    "\n",
    "test_text = \"Write a program for mod doubles b = 77 and c = doubles 77\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the nlp Object\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"./NLP_Object\")\n",
    "\n",
    "doc = nlp(\"create a function to add numbers a and b\")\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
