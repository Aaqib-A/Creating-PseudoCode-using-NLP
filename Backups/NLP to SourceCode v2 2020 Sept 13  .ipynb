{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rus first time to imporrt all files\n",
    "#import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datatypes = {\"integer\":\"int\", \"int\":\"int\",\n",
    "                 \"float\":\"float\", \"number\":\"float\", \"num\":\"float\",\n",
    "                 \"double\":\"double\",\n",
    "                 \"boolean\":\"bool\", \"bool\":\"bool\",\n",
    "                 \"character\":\"char\", \"char\":\"char\",\n",
    "                 \"string\":\"str\", \"str\":\"str\",\n",
    "                 \"void\":\"void\",\n",
    "                 \"array\":\"array\", \"arr\":\"array\", \"list\":\"array\", \"vector\":\"array\"}\n",
    "\n",
    "datatypes = stemming_dict(raw_datatypes)\n",
    "#print(datatypes)\n",
    "#print(datatypes.get('double',0))\n",
    "\n",
    "raw_operators = {\"add\":\"+\", \"addition\":\"+\", \"+\":\"+\",\n",
    "                 \"sub\":\"-\", \"subtract\":\"-\", \"subtraction\":\"-\", \"-\":\"-\", \"minus\":\"-\",\n",
    "                 \"mul\":\"*\", \"multiply\":\"*\", \"*\":\"*\",\n",
    "                 \"divide\":\"/\",\"div\":\"/\", \"/\":\"/\",\n",
    "                 \"mod\":\"%\", \"modulo\":\"%\", \"%\":\"%\"}\n",
    "\n",
    "operators = stemming_dict(raw_operators)\n",
    "#print(operators)\n",
    "\n",
    "raw_commands=['create', 'define', 'declare', 'set']\n",
    "\n",
    "raw_loops=['loop','loops'\n",
    "           'for loop', 'for loops','for',\n",
    "           'while loop','while loops', 'while',\n",
    "           'do while loop','do while loops', 'do while']\n",
    "\n",
    "custom_stoppers=['\\. ',' then ', ' than ']\n",
    "\n",
    "def extract_words_from_dict():\n",
    "    \n",
    "    list_list = [custom_stoppers,raw_commands,raw_loops]\n",
    "    dict_list = [raw_datatypes, raw_operators]\n",
    "    \n",
    "    vocab=set()\n",
    "    for lists in list_list:\n",
    "        for inner_list in lists:\n",
    "            vocab.add(inner_list)\n",
    "    for dicti in dict_list:\n",
    "        for keys in dicti.keys():\n",
    "            vocab.add(keys)\n",
    "    return vocab\n",
    "\n",
    "full_vocab = extract_words_from_dict()\n",
    "#print(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "#from nltk import word_tokenize \n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize1(statement1):   \n",
    "    return (word_tokenize(statement1))\n",
    "    '''\n",
    "    # instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                   reduce_len=True)\n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(statement)\n",
    "    return tweet_tokens\n",
    "    '''\n",
    "'''\n",
    "def tokenizer_custom_line(statement):\n",
    "    tokenize_statment = []\n",
    "    #tokenize_statment = re.findall(r'[(\\w )]* ',statement)\n",
    "    tokenize_statment = re.split(r\"\\. |\\, \", statement)\n",
    "    #print (tokenize_statment)\n",
    "    return (tokenize_statment)\n",
    "\n",
    "def tokenizer_custom_words(statement):\n",
    "    tokenize_statement = []\n",
    "    tokenize_statment = re.split(r\" \", statement)\n",
    "    return(tokenize_statment)\n",
    "'''\n",
    "\n",
    "def tokenizer_custom_line_stopper(statement):\n",
    "    tokenize_statement = []\n",
    "    \n",
    "    raw_string=\"\"\n",
    "    for index, stopper in enumerate(custom_stoppers): #'custom_stoppers' declareed above\n",
    "        if index==0:\n",
    "            raw_string = stopper\n",
    "        else:\n",
    "            raw_string = raw_string + '|' + stopper\n",
    "        \n",
    "    #print(\"RAW STRING:\")\n",
    "    #print(\"[\"+raw_string+\"]\")\n",
    "    tokenize_statment = re.split(r\"\"+raw_string, statement)\n",
    "    return (tokenize_statment)\n",
    "\n",
    "def tokenizer_custom_comma(statement):   \n",
    "    if \", \" in statement:\n",
    "        #Find \", \"\n",
    "        statement = statement.replace(', ', ',')\n",
    "        \n",
    "    if \",\" in statement:\n",
    "        #Find \",\"\n",
    "        comma_word = re.findall(r\"(,[^\\s]+)\", statement)\n",
    "        #print(\"Comma Word : \",set(comma_word))\n",
    "       \n",
    "        temp_list = []\n",
    "        s=[]\n",
    "        e=[]\n",
    "        s.append(0)\n",
    "        e.append(0)\n",
    "        index=1\n",
    "        for each_word in comma_word:\n",
    "            \n",
    "            if each_word in [','+vocab for vocab in full_vocab]:\n",
    "                #print (f\"Each Word: {each_word}\")\n",
    "                \n",
    "                for match in re.finditer(each_word, statement):\n",
    "                    s.append(match.start())\n",
    "                    e.append(match.end())\n",
    "                    #print (f\"Index[{index}] is from :{s[index]} {e[index]}\")    \n",
    "                    if (index==1):\n",
    "                        temp_list.append(statement[s[index-1]:s[index]])\n",
    "                    else:\n",
    "                        temp_list.append(statement[s[index-1]+1:s[index]])\n",
    "                    index +=1\n",
    "\n",
    "        if index !=1:\n",
    "            #print(statement[s[i+1]+1:])\n",
    "            temp_list.append(statement[s[index-1]+1:])  \n",
    "        #print (f\"temp_list: {temp_list}\")\n",
    "        if temp_list:\n",
    "            return (temp_list)         \n",
    "    #print (statement)\n",
    "\n",
    "    #statement = [re.sub(r\"(, |,)\", r\" \", statement)]\n",
    "    return (statement)\n",
    "        \n",
    "    \n",
    "def remove_extra_space(statement):\n",
    "    while \"  \" in statement:\n",
    "        statement = statement.replace(\"  \", \" \")\n",
    "    return (statement)\n",
    "\n",
    "def tokenize_sentence_advance(statement):\n",
    "    statement = remove_extra_space(statement)\n",
    "    statement = statement.lower()\n",
    "    \n",
    "    new_statement = tokenizer_custom_line_stopper(statement)\n",
    "    #print (new_statement)\n",
    "    \n",
    "    #temp_list=[]\n",
    "    final_statement=[]\n",
    "    for each_line in new_statement:\n",
    "        temp_list = tokenizer_custom_comma(each_line)\n",
    "        #print (\"Temp_list:\",temp_list)\n",
    "        if temp_list != each_line :\n",
    "            for each_line in temp_list:\n",
    "                final_statement.append(each_line)\n",
    "        else:\n",
    "            final_statement.append(each_line)\n",
    "    return (final_statement)\n",
    "\n",
    "#Testing\n",
    "'''\n",
    "#custom_statement_raw = \"define Variable a,b and 'c'. Add variables a,c and b, Subtract variables b from a then create a for loop and multiply a 5 times\"\n",
    "#custom_statement_raw = \"define Variable a and 'b'. Add variables a from b, Subtract variables b from a, create a for loop and multiply a 5 times then add 2 to a\"\n",
    "#custom_statement_raw = \"Define a and b then add 1 and 2 then create a for loop\"\n",
    "#custom_statement_raw = \"Define a and b,\"\n",
    "print(custom_statement_raw)\n",
    "print(tokenize_sentence_advance(custom_statement_raw))\n",
    "\n",
    "custom_statement_raw = remove_extra_space(custom_statement_raw)\n",
    "cust_statement1 = tokenizer_custom_line_stopper(custom_statement_raw)\n",
    "print(cust_statement1)\n",
    "for each_line in cust_statement1:\n",
    "    tokenizer_custom_comma(each_line)\n",
    "    #print (tokenizer_custom_words(each_line))\n",
    "'''\n",
    "print(\"\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence = [[]] *len(tokenize_sentence)\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for each_word in sentence:\n",
    "        if each_word not in stop_words:\n",
    "            filtered_sentence.append(each_word)\n",
    "    return (filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming_list(statement1):\n",
    "    # Instantiate stemming class\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # Create an empty list to store the stems\n",
    "    sentence_stem = [] \n",
    "\n",
    "    for word in statement1:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        sentence_stem.append(stem_word)  # append to the list\n",
    "\n",
    "    return(sentence_stem)\n",
    "\n",
    "def stemming_dict(dicti):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    sentence_stem={}\n",
    "    for word, value in dicti.items():\n",
    "        stem_word = stemmer.stem(word)\n",
    "        sentence_stem[stem_word] = value\n",
    "    return sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 5 times\n",
      "\n",
      "After Tokenization of Each Line\n",
      "[\"define variable var1 and 'var2'\", 'add variables var1 from var2', 'subtract variables var2 from var1', 'create a for loop and multiply var1 5 times']\n",
      "\n",
      "After Tokenization of Each Word\n",
      "[['define', 'variable', 'var1', 'and', \"'var2'\"], ['add', 'variables', 'var1', 'from', 'var2'], ['subtract', 'variables', 'var2', 'from', 'var1'], ['create', 'a', 'for', 'loop', 'and', 'multiply', 'var1', '5', 'times']]\n",
      "\n",
      "Removed Stopwords:\n",
      "[['define', 'variable', 'var1', \"'var2'\"], ['add', 'variables', 'var1', 'var2'], ['subtract', 'variables', 'var2', 'var1'], ['create', 'loop', 'multiply', 'var1', '5', 'times']]\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "#custom_statement_raw = \"define Variable a and 'b'. Add variables a from b, Subtract variables b from a, create a for loop and multiply a 5 times\"\n",
    "custom_statement_raw = \"define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 5 times\"\n",
    "print(custom_statement_raw)\n",
    "\n",
    "'''modified_statement = custom_statement_raw.lower()\n",
    "modified_statement = tokenizer_custom_line(modified_statement)  \n",
    "modified_statement2 = []\n",
    "for each_line in modified_statement:\n",
    "    modified_statement2.append(tokenizer_custom_words(each_line))\n",
    "print(\"\\nAfter Tokenizer:\")\n",
    "print(modified_statement2)\n",
    "                               \n",
    "#modified_statement = stemming_list(modified_statement)\n",
    "print(\"\\nAfter Porter Stemming:\")\n",
    "#print(modified_statement)\n",
    "modified_statement3 = []\n",
    "for each_line in modified_statement2:\n",
    "    modified_statement3.append(stemming_list(each_line))\n",
    "print(modified_statement3)\n",
    "'''\n",
    "tokenize_sentence = tokenize_sentence_advance(custom_statement_raw)\n",
    "print(\"\\nAfter Tokenization of Each Line\")\n",
    "print(tokenize_sentence)\n",
    "\n",
    "tokenize_words=[]\n",
    "for each_line in tokenize_sentence:\n",
    "    tokenize_words.append(tokenizer_custom_words(each_line))\n",
    "print(\"\\nAfter Tokenization of Each Word\")\n",
    "print(tokenize_words)\n",
    "\n",
    "\n",
    "filtered_sentence=[]\n",
    "for each_line in tokenize_words:\n",
    "    filtered_sentence.append(remove_stopwords(each_line) )\n",
    "print(\"\\nRemoved Stopwords:\")\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
