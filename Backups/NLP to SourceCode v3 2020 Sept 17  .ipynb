{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run first time to import all files\n",
    "#import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemming_list(statement1):\n",
    "    # Instantiate stemming class\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # Create an empty list to store the stems\n",
    "    sentence_stem = [] \n",
    "\n",
    "    for word in statement1:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        sentence_stem.append(stem_word)  # append to the list\n",
    "\n",
    "    return(sentence_stem)\n",
    "\n",
    "def stemming_dict(dicti):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    sentence_stem={}\n",
    "    for word, value in dicti.items():\n",
    "        stem_word = stemmer.stem(word)\n",
    "        sentence_stem[stem_word] = value\n",
    "    return sentence_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'add': '+', 'addit': '+', '+': '+', 'sub': '-', 'subtract': '-', '-': '-', 'minu': '-', 'mul': '*', 'multipli': '*', '*': '*', 'divid': '/', 'div': '/', '/': '/', 'mod': '%', 'modulo': '%', 'modulu': '%', '%': '%'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "raw_datatypes = {\"variable\":\"var\", \"var\":\"var\",\n",
    "                 \"variables\":\"var\", \"vars\":\"var\",\n",
    "                 \n",
    "                 \"integer\":\"int\", \"int\":\"int\",\n",
    "                 \"integers\":\"int\", \"ints\":\"int\",\n",
    "                 \n",
    "                 \"float\":\"float\", \"number\":\"float\",  \"num\":\"float\", \n",
    "                 \"floats\":\"float\", \"numbers\":\"float\",\"nums\":\"float\",\n",
    "                 \n",
    "                 \"double\":\"double\",\n",
    "                 \"doubles\":\"double\",\n",
    "                 \n",
    "                 \"boolean\":\"bool\", \"bool\":\"bool\", \n",
    "                 \"booleans\":\"bool\", \"bools\":\"bool\",\n",
    "                 \n",
    "                 \"character\":\"char\",  \"char\":\"char\", \n",
    "                 \"characters\":\"char\", \"chars\":\"char\",\n",
    "                 \n",
    "                 \"string\":\"str\", \"str\":\"str\", \n",
    "                 \"strings\":\"str\", \"strs\":\"str\",\n",
    "                 \n",
    "                 \"void\":\"void\",\n",
    "                 \"voids\":\"void\",\n",
    "                 \n",
    "                 \"array\":\"array\", \"arr\":\"array\", \"list\":\"array\", \"vector\":\"array\",\n",
    "                 \"arrays\":\"array\", \"arrs\":\"array\", \"lists\":\"array\", \"vectors\":\"array\"}\n",
    "\n",
    "datatypes = stemming_dict(raw_datatypes)\n",
    "#print(datatypes)\n",
    "#print(datatypes.get('double',0))\n",
    "\n",
    "raw_operators = {\"add\":\"+\", \"addition\":\"+\", \"+\":\"+\",\n",
    "                 \"sub\":\"-\", \"subtract\":\"-\", \"subtraction\":\"-\", \"-\":\"-\", \"minus\":\"-\",\n",
    "                 \"mul\":\"*\", \"multiply\":\"*\", \"*\":\"*\",\n",
    "                 \"divide\":\"/\",\"div\":\"/\", \"/\":\"/\",\n",
    "                 \"mod\":\"%\", \"modulo\":\"%\", \"modulus\":\"%\", \"%\":\"%\"}\n",
    "\n",
    "operators = stemming_dict(raw_operators)\n",
    "#print(operators)\n",
    "\n",
    "raw_commands=['create', 'define', 'declare', 'set']\n",
    "\n",
    "raw_loops=['loop','loops'\n",
    "           'for loop', 'for loops','for',\n",
    "           'while loop','while loops', 'while',\n",
    "           'do while loop','do while loops', 'do while']\n",
    "\n",
    "raw_others=[\"time\", \n",
    "            \"times\"]\n",
    "\n",
    "custom_stoppers=['\\. ',' then ', ' than ']\n",
    "\n",
    "def extract_words_from_dict():\n",
    "    \n",
    "    list_list = [custom_stoppers,raw_commands,raw_loops, raw_others]\n",
    "    dict_list = [raw_datatypes, raw_operators]\n",
    "    \n",
    "    vocab=set()\n",
    "    for lists in list_list:\n",
    "        for inner_list in lists:\n",
    "            vocab.add(inner_list)\n",
    "    for dicti in dict_list:\n",
    "        for keys in dicti.keys():\n",
    "            vocab.add(keys)\n",
    "    return vocab\n",
    "\n",
    "full_vocab = extract_words_from_dict()\n",
    "#print(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "\n",
    "#from nltk import word_tokenize \n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize1(statement1):   \n",
    "    return (word_tokenize(statement1))\n",
    "    '''\n",
    "    # instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                                   reduce_len=True)\n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(statement)\n",
    "    return tweet_tokens\n",
    "    '''\n",
    "'''\n",
    "def tokenizer_custom_line(statement):\n",
    "    tokenize_statment = []\n",
    "    #tokenize_statment = re.findall(r'[(\\w )]* ',statement)\n",
    "    tokenize_statment = re.split(r\"\\. |\\, \", statement)\n",
    "    #print (tokenize_statment)\n",
    "    return (tokenize_statment)\n",
    "'''\n",
    "def tokenizer_custom_words(statement):\n",
    "    tokenize_statement = []\n",
    "    tokenize_statment = re.split(r\" \", statement)\n",
    "    return(tokenize_statment)\n",
    "\n",
    "\n",
    "def tokenizer_custom_line_stopper(statement):\n",
    "    tokenize_statement = []\n",
    "    \n",
    "    raw_string=\"\"\n",
    "    for index, stopper in enumerate(custom_stoppers): #'custom_stoppers' declareed above\n",
    "        if index==0:\n",
    "            raw_string = stopper\n",
    "        else:\n",
    "            raw_string = raw_string + '|' + stopper\n",
    "        \n",
    "    #print(\"RAW STRING:\")\n",
    "    #print(\"[\"+raw_string+\"]\")\n",
    "    tokenize_statment = re.split(r\"\"+raw_string, statement)\n",
    "    return (tokenize_statment)\n",
    "\n",
    "def tokenizer_custom_comma(statement):   \n",
    "    if \", \" in statement:\n",
    "        #Find \", \"\n",
    "        statement = statement.replace(', ', ',')\n",
    "        \n",
    "    if \",\" in statement:\n",
    "        #Find \",\"\n",
    "        comma_word = re.findall(r\"(,[^\\s]+)\", statement)\n",
    "        #print(\"Comma Word : \",set(comma_word))\n",
    "       \n",
    "        temp_list = []\n",
    "        s=[]\n",
    "        e=[]\n",
    "        s.append(0)\n",
    "        e.append(0)\n",
    "        index=1\n",
    "        for each_word in comma_word:\n",
    "            \n",
    "            if each_word in [','+vocab for vocab in full_vocab]:\n",
    "                #print (f\"Each Word: {each_word}\")\n",
    "                \n",
    "                for match in re.finditer(each_word, statement):\n",
    "                    s.append(match.start())\n",
    "                    e.append(match.end())\n",
    "                    #print (f\"Index[{index}] is from :{s[index]} {e[index]}\")    \n",
    "                    if (index==1):\n",
    "                        temp_list.append(statement[s[index-1]:s[index]])\n",
    "                    else:\n",
    "                        temp_list.append(statement[s[index-1]+1:s[index]])\n",
    "                    index +=1\n",
    "\n",
    "        if index !=1:\n",
    "            #print(statement[s[i+1]+1:])\n",
    "            temp_list.append(statement[s[index-1]+1:])  \n",
    "        #print (f\"temp_list: {temp_list}\")\n",
    "        if temp_list:\n",
    "            return (temp_list)         \n",
    "    #print (statement)\n",
    "\n",
    "    #statement = [re.sub(r\"(, |,)\", r\" \", statement)]\n",
    "    return (statement)\n",
    "\n",
    "    \n",
    "def remove_extra_space(statement):\n",
    "    while \"  \" in statement:\n",
    "        statement = statement.replace(\"  \", \" \")\n",
    "    return (statement)\n",
    "'''\n",
    "def tokenize_sentence_advance(statement):\n",
    "    statement = remove_extra_space(statement)\n",
    "    statement = statement.lower()\n",
    "    \n",
    "    new_statement = tokenizer_custom_line_stopper(statement)\n",
    "    #print (new_statement)\n",
    "    \n",
    "    #temp_list=[]\n",
    "    final_statement=[]\n",
    "    for each_line in new_statement:\n",
    "        temp_list = tokenizer_custom_comma(each_line)\n",
    "        #print (\"Temp_list:\",temp_list)\n",
    "        if temp_list != each_line :\n",
    "            for each_line in temp_list:\n",
    "                final_statement.append(each_line)\n",
    "        else:\n",
    "            final_statement.append(each_line)\n",
    "    return (final_statement)\n",
    "'''\n",
    "\n",
    "def tokenize_sentence_advance(statement):\n",
    "    statement = remove_extra_space(statement)\n",
    "    statement = statement.lower()\n",
    "    \n",
    "    new_statement = tokenizer_custom_line_stopper(statement)\n",
    "    #print (new_statement)\n",
    "    \n",
    "    #temp_list=[]\n",
    "    final_statement=[]\n",
    "    for each_line in new_statement:\n",
    "        temp_list = tokenizer_custom_comma(each_line)\n",
    "        #print (\"Temp_list:\",temp_list)\n",
    "        if temp_list != each_line :\n",
    "            for each_line in temp_list:\n",
    "                final_statement.append(each_line)\n",
    "        else:\n",
    "            final_statement.append(each_line)\n",
    "    return (final_statement)\n",
    "\n",
    "#Testing\n",
    "'''\n",
    "#custom_statement_raw = \"define Variable a,b and 'c'. Add variables a,c and b, Subtract variables b from a then create a for loop and multiply a 5 times\"\n",
    "#custom_statement_raw = \"define Variable a and 'b'. Add variables a from b, Subtract variables b from a, create a for loop and multiply a 5 times then add 2 to a\"\n",
    "#custom_statement_raw = \"Define a and b then add 1 and 2 then create a for loop\"\n",
    "#custom_statement_raw = \"Define a and b,\"\n",
    "print(custom_statement_raw)\n",
    "print(tokenize_sentence_advance(custom_statement_raw))\n",
    "\n",
    "'''\n",
    "print(\"\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "#stop_words = set(stopwords.words('english')) \n",
    "#filtered_sentence = [[]] *len(tokenize_sentence)\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = []\n",
    "    for each_word in sentence:\n",
    "        if each_word not in stop_words:\n",
    "            filtered_sentence.append(each_word)\n",
    "    return (filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Known Words to get variables\n",
    "def remove_known_words(sentence, known_words=full_vocab):\n",
    "    variables_list= []\n",
    "    for each_word in sentence:\n",
    "        if (each_word not in known_words and not each_word.isdigit()):\n",
    "            variables_list.append(each_word)\n",
    "    return variables_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 5 times \""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text to numbers\n",
    "def text2int (textnum, numwords={}):\n",
    "    if not numwords:\n",
    "        units = [\n",
    "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "        ]\n",
    "\n",
    "        tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "        scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "        #numwords[\"and\"] = (1, 0)\n",
    "        for idx, word in enumerate(units):  numwords[word] = (1, idx)\n",
    "        for idx, word in enumerate(tens):       numwords[word] = (1, idx * 10)\n",
    "        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}\n",
    "    ordinal_endings = [('ieth', 'y'), ('th', '')]\n",
    "\n",
    "    textnum = textnum.replace('-', ' ')\n",
    "\n",
    "    current = result = 0\n",
    "    curstring = \"\"\n",
    "    onnumber = False\n",
    "    for word in textnum.split():\n",
    "        if word in ordinal_words:\n",
    "            scale, increment = (1, ordinal_words[word])\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "            onnumber = True\n",
    "        else:\n",
    "            for ending, replacement in ordinal_endings:\n",
    "                if word.endswith(ending):\n",
    "                    word = \"%s%s\" % (word[:-len(ending)], replacement)\n",
    "\n",
    "            if word not in numwords:\n",
    "                if onnumber:\n",
    "                    curstring += repr(result + current) + \" \"\n",
    "                curstring += word + \" \"\n",
    "                result = current = 0\n",
    "                onnumber = False\n",
    "            else:\n",
    "                scale, increment = numwords[word]\n",
    "\n",
    "                current = current * scale + increment\n",
    "                if scale > 100:\n",
    "                    result += current\n",
    "                    current = 0\n",
    "                onnumber = True\n",
    "\n",
    "    if onnumber:\n",
    "        curstring += repr(result + current)\n",
    "    return curstring\n",
    "\n",
    "    \n",
    "text2int(\"define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 five times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 5 times\n",
      "\n",
      "After Tokenization of Each Line\n",
      "[\"define variable var1 and 'var2'\", 'add variables var1 from var2', 'subtract variables var2 from var1', 'create a for loop and multiply var1 5 times']\n",
      "\n",
      "After Tokenization of Each Word\n",
      "[['define', 'variable', 'var1', 'and', \"'var2'\"], ['add', 'variables', 'var1', 'from', 'var2'], ['subtract', 'variables', 'var2', 'from', 'var1'], ['create', 'a', 'for', 'loop', 'and', 'multiply', 'var1', '5', 'times']]\n",
      "\n",
      "Removed Stopwords:\n",
      "[['define', 'variable', 'var1', \"'var2'\"], ['add', 'variables', 'var1', 'var2'], ['subtract', 'variables', 'var2', 'var1'], ['create', 'loop', 'multiply', 'var1', 'times']]\n",
      "\n",
      "Variables List:\n",
      "[['var1', \"'var2'\"], ['var1', 'var2'], ['var2', 'var1'], ['var1']]\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "#custom_statement_raw = \"define Variable a and 'b'. Add variables a from b, Subtract variables b from a, create a for loop and multiply a 5 times\"\n",
    "custom_statement_raw = \"define Variable var1 and 'var2'. Add variables var1 from var2, Subtract variables var2 from var1, create a for loop and multiply var1 5 times\"\n",
    "print(custom_statement_raw)\n",
    "\n",
    "\n",
    "tokenize_sentence = tokenize_sentence_advance(custom_statement_raw)\n",
    "print(\"\\nAfter Tokenization of Each Line\")\n",
    "print(tokenize_sentence)\n",
    "\n",
    "tokenize_words=[]\n",
    "for each_line in tokenize_sentence:\n",
    "    tokenize_words.append(tokenizer_custom_words(each_line))\n",
    "print(\"\\nAfter Tokenization of Each Word\")\n",
    "print(tokenize_words)\n",
    "\n",
    "\n",
    "filtered_sentence=[]\n",
    "for each_line in tokenize_words:\n",
    "    filtered_sentence.append(remove_stopwords(each_line) )\n",
    "print(\"\\nRemoved Stopwords:\")\n",
    "print(filtered_sentence)\n",
    "\n",
    "variables_list = []\n",
    "print(\"\\nVariables List:\")\n",
    "for each_line in filtered_sentence:\n",
    "    variables_list.append(remove_known_words(each_line))\n",
    "print(variables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
