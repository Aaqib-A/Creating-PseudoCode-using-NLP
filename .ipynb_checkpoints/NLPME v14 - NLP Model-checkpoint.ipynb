{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing importing Files\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading File Data\n",
    "\n",
    "fileData=[]\n",
    "import os\n",
    "\n",
    "dest_folder = \"NLP_ModelData_v14\"\n",
    "input_file_loc = os.path.join(dest_folder, \"Dataset_DataGenerator.txt\")\n",
    "\n",
    "with open(input_file_loc, \"r\") as file:\n",
    "    for line in file:\n",
    "        x = re.sub(\"=\", \" = \", line)\n",
    "        x = re.sub(\"  \",\" \", x)   \n",
    "        fileData.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Shelf Data from \"Data Generator\"\n",
    "import shelve\n",
    "import os\n",
    "\n",
    "dest_file = os.path.join(dest_folder, \"Dict_Lists.shlf\")\n",
    "\n",
    "shelf = shelve.open(dest_file)\n",
    "\n",
    "dataType_number = shelf[\"dataType_number\"]\n",
    "dataType_char = shelf[\"dataType_char\"]\n",
    "dataType_list = shelf[\"dataType_list\"]\n",
    "dataType_bool = shelf[\"dataType_bool\"]\n",
    "dataType_class = shelf[\"dataType_class\"]\n",
    "dataType_void = shelf[\"dataType_void\"]\n",
    "dataType = shelf[\"dataType\"]\n",
    "\n",
    "variables_list = shelf[\"variables_list\"]\n",
    "func_name = shelf[\"func_name\"]\n",
    "\n",
    "scope_of_variable = shelf[\"scope_of_variable\"]\n",
    "\n",
    "operators = shelf[\"operators\"]\n",
    "\n",
    "command_declare = shelf[\"command_declare\"]\n",
    "command_print = shelf[\"command_print\"]\n",
    "command_input = shelf[\"command_input\"]\n",
    "\n",
    "func_prog = shelf[\"func_prog\"]\n",
    "\n",
    "special_class = shelf[\"special_class\"]\n",
    "special_function = shelf[\"special_function\"]\n",
    "\n",
    "other_tags = shelf[\"other_tags\"]\n",
    "seperators = shelf[\"seperators\"] \n",
    "equal_symbol = shelf[\"equal_symbol\"]\n",
    "\n",
    "\n",
    "shelf.close()\n",
    "\n",
    "#print(dataType_number, \"\\n\")\n",
    "#print(dataType, \"\\n\")\n",
    "#print(variables_list, \"\\n\")\n",
    "#print(scope_of_variable, \"\\n\")\n",
    "#print(operators, \"\\n\")\n",
    "#print(command_declare, \"\\n\")\n",
    "#print(command_print, \"\\n\")\n",
    "#print(command_input, \"\\n\")\n",
    "#print(func_prog, \"\\n\")\n",
    "#print(special_class, \"\\n\")\n",
    "#print(special_function, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Patterns\n",
    "#{\"IN\":[\"\",\"s\"]}\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1_1 = [{\"LOWER\":{\"IN\":dataType}}]\n",
    "matcher.add(\"DATATYPE\", None, pattern1_1)\n",
    "\n",
    "pattern2_1 = [{\"LOWER\":\"global\"}]\n",
    "pattern2_2 = [{\"LOWER\":\"local\"}]\n",
    "matcher.add(\"SCOPE\", None, pattern2_1, pattern2_2)\n",
    "\n",
    "pattern3_1 = [{\"LOWER\":{\"IN\":operators}}]\n",
    "pattern3_2 = [{\"LOWER\":{\"IN\":[\"additions\", \"adds\"]}}]\n",
    "pattern3_3 = [{\"LOWER\":{\"IN\":[\"subtractions\", \"subs\"]}}]\n",
    "pattern3_4 = [{\"LOWER\":{\"IN\":[\"minuses\", \"multiplies\", \"muls\"]}}]\n",
    "pattern3_5 = [{\"LOWER\":{\"IN\":[\"divides\", \"divs\"]}}]\n",
    "pattern3_6 = [{\"LOWER\":{\"IN\":[\"modulus\", \"moduluses\", \"mods\"]}}]\n",
    "matcher.add(\"OPERATOR\", None, pattern3_1, pattern3_2, pattern3_3, pattern3_4, pattern3_5, pattern3_6)\n",
    "\n",
    "pattern4_1 = [{\"LOWER\":{\"IN\":command_declare}}]\n",
    "matcher.add(\"INIT_COMMAND\", None, pattern4_1)\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":command_print}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":command_input}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":special_function}}]\n",
    "matcher.add(\"COMMAND\", None, pattern5_1, pattern5_2, pattern5_3)\n",
    "\n",
    "pattern6_1 = [{\"LOWER\":{\"IN\":func_prog}}]\n",
    "pattern6_2 = [{\"LOWER\":{\"IN\":[\"funcs\",\"functions\"]}}]\n",
    "pattern6_3 = [{\"LOWER\":{\"IN\":[\"programs\",\"programmes\",\"progs\", \"prgs\"]}}]\n",
    "matcher.add(\"FUNC_PROG\", None, pattern6_1, pattern6_2, pattern6_3)\n",
    "\n",
    "pattern7_1 = [{\"LOWER\": {\"IN\": variables_list}}]\n",
    "matcher.add(\"VARI\", None, pattern7_1)\n",
    "\n",
    "pattern8_1 = [{\"LOWER\": {\"IN\": special_class}}]\n",
    "matcher.add(\"SPL_CLASS\", None, pattern8_1)\n",
    "\n",
    "pattern9_1 = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"VALUE\", None, pattern9_1)\n",
    "\n",
    "pattern10_1 = [{\"LOWER\": {\"IN\": seperators}}]\n",
    "matcher.add(\"SEPERATOR\", None, pattern10_1)\n",
    "\n",
    "#pattern10_1 = [{\"LOWER\":{\"IN\":equal_symbol}}, {\"ORTH\": \"to\", \"OP\":\"?\"}]\n",
    "#pattern10_1 = [{\"TEXT\": {\"REGEX\": u\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"}}]\n",
    "#pattern10_2 = [{\"TEXT\": {\"REGEX\": \"[Ee][Qq][Uu][Aa][Ll][Ss]?\"}}, {\"ORTH\": {\"REGEX\": \" [Tt][Oo]\"}, \"OP\":\"?\" }]\n",
    "#pattern10_2 = [{\"LOWER\":\"equals\"}, {\"TEXT\": {\"REGEX\": \"( [Tt][Oo])\"}, \"OP\":\"?\"}]\n",
    "#pattern10_3 = [{\"LOWER\":\"equal\"}, {\"TEXT\": {\"REGEX\": \"( [Tt][Oo])\"}, \"OP\":\"?\"}]\n",
    "#matcher.add(\"EQUAL\", None, pattern10_1, pattern10_2)#, pattern10_3)#, pattern10_4, pattern10_5)\n",
    "pattern10_1 = [{\"ORTH\":\"=\"}]\n",
    "matcher.add(\"EQUAL\", None, pattern10_1)\n",
    "\n",
    "pattern11_1 = [{\"ORTH\": \"'\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"'\"}]\n",
    "pattern11_2 = [{\"ORTH\": \"\\\"\"}, {\"TEXT\": {\"REGEX\": \".\"}, \"OP\":\"*\"},{\"ORTH\": \"\\\"\"}]\n",
    "matcher.add(\"TEXT\", None, pattern11_1, pattern11_2)\n",
    "\n",
    "pattern12_1 = [{\"LOWER\": {\"IN\": func_name}}]\n",
    "matcher.add(\"FUNC_NAME\", None, pattern12_1)\n",
    "\n",
    "#pattern12_1 = [{\"ORTH\": \"two\"}]\n",
    "#pattern12_2 = [{\"ORTH\": \"3\"}]\n",
    "#matcher.add(\"COUNT\", None, pattern12_1, pattern12_2)\n",
    "\n",
    "#pattern12_1 = [{\"LOWER\": {\"IN\": other_tags}}]\n",
    "#matcher.add(\"OTHER\", None, pattern12_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 3770598 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7d075dcdb83c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Create a Doc object for each text in TEXTS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#x = re.sub(\"=\", \" = \", doc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mpipe\u001b[1;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[0;32m    827\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    830\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36mpipe\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mminibatch\u001b[1;34m(items, size)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[1;31m# if n_process == 1, no processes are forked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m             \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                 \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    465\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 3770598 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "#Creating Training Data\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(fileData):\n",
    "    \n",
    "    #x = re.sub(\"=\", \" = \", doc)\n",
    "    #doc = re.sub(\"  \",\" \", x)\n",
    "    \n",
    "    '''\n",
    "    #Regex Matcher - equals to\n",
    "    expression = r\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        span.append(doc.char_span(start, end))\n",
    "        entity.append(\"EQUAL\")\n",
    "        # This is a Span object or None if match doesn't map to valid token sequence\n",
    "        #if span is not None:\n",
    "        #    print(\"Found match:\", span.text, span.start_char, span.end_char)\n",
    "    '''\n",
    "     \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    #print(spans)\n",
    "    \n",
    "    #Modified Start\n",
    "    entity_label=[]\n",
    "    entity_label = [(nlp.vocab.strings[match_id]) for match_id, start, end in matcher(doc)]\n",
    "    #print(entity_label)\n",
    "    \n",
    "    #Regex Matcher - equals to\n",
    "    expression = r\"[Ee][Qq][Uu][Aa][Ll][Ss]?( [Tt][Oo])?\"\n",
    "    for match in re.finditer(expression, doc.text):\n",
    "        start, end = match.span()\n",
    "        #span = doc.char_span(start, end)\n",
    "        spans.append(doc.char_span(start, end))\n",
    "        entity_label.append(\"EQUAL\")\n",
    "    \n",
    "\n",
    "    entities = [(span.start_char, span.end_char, entity_label[index]) for index,span in enumerate(spans)]\n",
    "    #print(entities)\n",
    "    #tags = [(entity_label[index]) for index,span in enumerate(spans)]\n",
    "    \n",
    "    #for eachWord in entities:\n",
    "    #    print(\"Entities: {}\".format(eachWord))\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    #training_example = (doc.text, {\"entities\": entities}, {\"tags\": tags})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    #Modified End\n",
    "    \n",
    "    '''\n",
    "    entities = [(span.start_char, span.end_char, \"INITIALIZATION\") for span in spans]\n",
    "    \n",
    "    for eachWord in entities:\n",
    "        print(\"Entities: {}\".format(eachWord))\n",
    "        \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    '''\n",
    "    \n",
    "\n",
    "#print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store to Data file for inspection\n",
    "\n",
    "train_data_loc = os.path.join(dest_folder, \"training_data.txt\")\n",
    "with open(train_data_loc,\"w\") as o_file:\n",
    "    for item in TRAINING_DATA:\n",
    "        o_file.write(\"{} \\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Pickle Data\n",
    "import pickle\n",
    "\n",
    "train_pickle_loc = os.path.join(dest_folder, \"training_data.pkl\")\n",
    "\n",
    "with open(train_pickle_loc, 'wb') as f:\n",
    "    pickle.dump(TRAINING_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Pickle data\n",
    "with open(train_pickle_loc, 'rb') as f:\n",
    "    TRAINING_DATA = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W033] Training a new parser or NER using a model with an empty lexeme normalization table. This may degrade the performance to some degree. If this is intentional or this language doesn't have a normalization table, please ignore this warning.\n",
      "  proc.begin_training(\n",
      "D:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py:635: UserWarning: [W034] Please install the package spacy-lookups-data in order to include the default lexeme normalization table for the language 'en'.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 149273.40488099298}\n",
      "{'ner': 4.33002071098546}\n",
      "{'ner': 1.9517166594428954}\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "import random\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "#tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(tagger) \n",
    "\n",
    "\n",
    "ner.add_label(\"DATATYPE\")\n",
    "ner.add_label(\"SCOPE\")\n",
    "ner.add_label(\"OPERATOR\")\n",
    "ner.add_label(\"INIT_COMMAND\")\n",
    "ner.add_label(\"COMMAND\")\n",
    "ner.add_label(\"FUNC_PROG\")\n",
    "ner.add_label(\"FUNC_NAME\")\n",
    "ner.add_label(\"VARI\")\n",
    "ner.add_label(\"SPL_CLASS\")\n",
    "ner.add_label(\"VALUE\")\n",
    "ner.add_label(\"COUNT\")\n",
    "ner.add_label(\"SEPERATOR\")\n",
    "ner.add_label(\"EQUAL\")\n",
    "ner.add_label(\"TEXT\")\n",
    "#ner.add_label(\"OTHER\")\n",
    "\n",
    "'''\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"FUNC_NAME\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "tagger.add_label(\"COUNT\")\n",
    "tagger.add_label(\"SEPERATOR\")\n",
    "tagger.add_label(\"EQUAL\")\n",
    "tagger.add_label(\"TEXT\")\n",
    "tagger.add_label(\"OTHER\")\n",
    "'''\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(3):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1000):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        #for text, entities, tags in batch:\n",
    "        #    annotations.append(entities)   \n",
    "        #    annotations.append(tags)\n",
    "        \n",
    "        #print(texts)\n",
    "        #print(annotations)\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "               \n",
    "        #annotations_tag = [tags for text, entities, tags in batch]\n",
    "        #print(annotations_tag)\n",
    "        #nlp.update(texts, annotations_tag, losses=losses)\n",
    "        \n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 0 3 COMMAND\n",
      "input 4 9 COMMAND\n",
      "k 10 11 VARI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    get\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COMMAND</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    input\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">COMMAND</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    k\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">VARI</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NER Test\n",
    "from spacy import displacy\n",
    "\n",
    "#doc = (\"create a function to add numbers z and b\")\n",
    "#doc = (\"if a > b then add a and b\")\n",
    "doc = (\"get input k\")\n",
    "#doc = (\"if a>b then print a. If b>c then print c\")\n",
    "\n",
    "#doc = (\"display variable text1 equals to 'Wubba lubba dubdub! General Kenobi A sentence' on screen\")\n",
    "#doc = (\"Create a program and name it wazza_wazza for create local integer equal to 45\")\n",
    "\n",
    "#doc = re.sub(\"=\", \" = \", doc)\n",
    "#doc = re.sub(\"  \",\" \", doc) \n",
    "\n",
    "doc = nlp(doc)\n",
    "  \n",
    "\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('Write', '', ''), ('a', '', ''), ('program', '', ''), ('for', '', ''), ('mod', '', ''), ('doubles', '', ''), ('b', '', ''), ('=', '', ''), ('77', '', ''), ('and', '', ''), ('c', '', ''), ('=', '', ''), ('doubles', '', ''), ('77', '', '')]\n"
     ]
    }
   ],
   "source": [
    "# Tagger Test\n",
    "\n",
    "test_text = \"Write a program for mod doubles b = 77 and c = doubles 77\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the nlp Object\n",
    "nlp.to_disk(\"./NLP_Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the nlp Object\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"./NLP_Object\")\n",
    "\n",
    "doc = nlp(\"create a function to add numbers a and b\")\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "displacy.render(nlp(doc.text), style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
