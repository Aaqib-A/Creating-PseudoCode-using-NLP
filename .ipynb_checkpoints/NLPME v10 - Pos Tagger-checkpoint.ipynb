{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileData=[]\n",
    "with open(\"Output.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        x = re.sub(\"=\", \" = \", line)\n",
    "        x = re.sub(\"  \",\" \", x)   \n",
    "        fileData.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import os\n",
    "\n",
    "dest_file=\"NLP_Shelfv8//\" + \"Dict_Lists\" + \".shlf\"\n",
    "\n",
    "shelf = shelve.open(dest_file)\n",
    "\n",
    "dataType_number = shelf[\"dataType_number\"]\n",
    "dataType_char = shelf[\"dataType_char\"]\n",
    "dataType_list = shelf[\"dataType_list\"]\n",
    "dataType_bool = shelf[\"dataType_bool\"]\n",
    "dataType_class = shelf[\"dataType_class\"]\n",
    "dataType_void = shelf[\"dataType_void\"]\n",
    "dataType = shelf[\"dataType\"]\n",
    "\n",
    "variables_list = shelf[\"variables_list\"]\n",
    "\n",
    "scope_of_variable = shelf[\"scope_of_variable\"]\n",
    "\n",
    "operators = shelf[\"operators\"]\n",
    "\n",
    "command_declare = shelf[\"command_declare\"]\n",
    "command_print = shelf[\"command_print\"]\n",
    "command_input = shelf[\"command_input\"]\n",
    "\n",
    "func_prog = shelf[\"func_prog\"]\n",
    "\n",
    "special_class = shelf[\"special_class\"]\n",
    "special_function = shelf[\"special_function\"]\n",
    "\n",
    "shelf.close()\n",
    "\n",
    "func_prog_start = [\n",
    "                 \"Write a program to\", \"Create a program to\", \"Write a program for\", \"Create a program for\",\n",
    "                 \"Write a function to\", \"Create a function to\",\"Write a function for\", \"Create a function for\",\n",
    "    \n",
    "                 \"Write program to\",  \"Create program for\",\n",
    "                 \"Write function to\", \"Create function for\",\n",
    "                 \"\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Patterns\n",
    "#{\"IN\":[\"\",\"s\"]}\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1_1 = [{\"LOWER\":{\"IN\":dataType}}]\n",
    "matcher.add(\"DATATYPE\", None, pattern1_1)\n",
    "\n",
    "pattern2_1 = [{\"LOWER\":\"global\"}]\n",
    "pattern2_2 = [{\"LOWER\":\"local\"}]\n",
    "matcher.add(\"SCOPE\", None, pattern2_1, pattern2_2)\n",
    "\n",
    "pattern3_1 = [{\"LOWER\":{\"IN\":operators}}]\n",
    "pattern3_2 = [{\"LOWER\":{\"IN\":[\"additions\", \"adds\"]}}]\n",
    "pattern3_3 = [{\"LOWER\":{\"IN\":[\"subtractions\", \"subs\"]}}]\n",
    "pattern3_4 = [{\"LOWER\":{\"IN\":[\"minuses\", \"multiplies\", \"muls\"]}}]\n",
    "pattern3_5 = [{\"LOWER\":{\"IN\":[\"divides\", \"divs\"]}}]\n",
    "pattern3_6 = [{\"LOWER\":{\"IN\":[\"modulus\", \"moduluses\", \"mods\"]}}]\n",
    "matcher.add(\"OPERATOR\", None, pattern3_1, pattern3_2, pattern3_3, pattern3_4, pattern3_5, pattern3_6)\n",
    "\n",
    "pattern4_1 = [{\"LOWER\":{\"IN\":command_declare}}]\n",
    "pattern4_2 = [{\"LOWER\":{\"IN\":command_print}}]\n",
    "pattern4_3 = [{\"LOWER\":{\"IN\":command_input}}]\n",
    "pattern4_4 = [{\"LOWER\":{\"IN\":special_function}}]\n",
    "matcher.add(\"COMMAND\", None, pattern4_1, pattern4_2, pattern4_3, pattern4_4)\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":[\"function\",\"functions\"]}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"func\",\"funcs\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"program\",\"programs\"]}}]\n",
    "pattern5_4 = [{\"LOWER\":{\"IN\":[\"programme\",\"programmes\"]}}]\n",
    "pattern5_5 = [{\"LOWER\":{\"IN\":[\"prog\",\"progs\"]}}]\n",
    "\n",
    "pattern5_1 = [{\"LOWER\":{\"IN\":func_prog}}]\n",
    "pattern5_2 = [{\"LOWER\":{\"IN\":[\"funcs\",\"functions\"]}}]\n",
    "pattern5_3 = [{\"LOWER\":{\"IN\":[\"programs\",\"programmes\",\"progs\"]}}]\n",
    "matcher.add(\"FUNC_PROG\", None, pattern5_1, pattern5_2, pattern5_3)\n",
    "\n",
    "pattern6_1 = [{\"LOWER\": {\"IN\": variables_list}}]\n",
    "matcher.add(\"VARI\", None, pattern6_1)\n",
    "\n",
    "pattern7_1 = [{\"LOWER\": {\"IN\": special_class}}]\n",
    "matcher.add(\"SPL_CLASS\", None, pattern7_1)\n",
    "\n",
    "pattern8_1 = [{\"LIKE_NUM\": True}]\n",
    "matcher.add(\"VALUE\", None, pattern8_1)\n",
    "\n",
    "pattern9_1 = [{\"LOWER\": {\"IN\": [\"and\", ',']}}]\n",
    "matcher.add(\"SEPERATOR\", None, pattern9_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create variables\n",
    "\n",
    "def generate_variable(dataFill, append_extra=False):\n",
    "    return_list = []\n",
    "    s_included = False\n",
    "    \n",
    "    for each_dataFill in dataFill:\n",
    "        if (s_included==False):\n",
    "                for i in range(0,3):\n",
    "                    return_list.append(each_dataFill+str(i))\n",
    "                    return_list.append(each_dataFill+\"_\"+str(i))\n",
    "                    s_included = True\n",
    "                '''\n",
    "                for i in range(10,13):\n",
    "                    return_list.append(each_dataFill+str(i))\n",
    "                    return_list.append(each_dataFill+\"_\"+str(i))\n",
    "                    s_included = True\n",
    "                '''\n",
    "                \n",
    "        else:\n",
    "            s_included = False\n",
    "            \n",
    "    if (append_extra==True):        \n",
    "        return_list.append(\"i\")\n",
    "        return_list.append(\"j\")\n",
    "        return_list.append(\"k\")\n",
    "        #return_list.append(\"a\")\n",
    "        return_list.append(\"b\")\n",
    "        return_list.append(\"c\")\n",
    "        \n",
    "    return (return_list)\n",
    "            \n",
    "#print (variables_list)\n",
    "#print(generate_variable(dataType))\n",
    "#print(generate_variable(dataType_char))\n",
    "#print(generate_variable(dataType_list, True))\n",
    "#print(generate_variable([\"var\"]))\n",
    "\n",
    "variables_list = generate_variable(dataType, True)\n",
    "#print (variables_list)\n",
    "\n",
    "def generate_partial_data(txt_code, datafill_list, input_list):\n",
    "        \n",
    "    returnList = []\n",
    "    \n",
    "    for eachLine in input_list:\n",
    "        if txt_code in eachLine:\n",
    "            for eachData in datafill_list:\n",
    "                tmp_txt = eachLine.replace(txt_code, eachData)\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                returnList.append(tmp_txt)\n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "    '''\n",
    "    generate_variable()\n",
    "    for eachLine in input_txt:\n",
    "        if \"SCOPE_OFVAR\" in eachLine:\n",
    "            for each_scope in scope_of_variable:\n",
    "                tmp_txt = eachLine.replace(\"SCOPE_OFVAR\", each_scope)\n",
    "                output_list.append( tmp_txt )\n",
    "        else:\n",
    "            output_list.append(eachLine)\n",
    "    '''\n",
    "\n",
    "def generate_partial_data_DTYPE(txt_code, datafill_list, input_list):\n",
    "        \n",
    "    returnList = []\n",
    "    \n",
    "    for eachLine in input_list:\n",
    "        \n",
    "        if txt_code in eachLine:\n",
    "            for eachData in datafill_list:\n",
    "                temp_returnList = []\n",
    "                tmp_txt = eachLine.replace(txt_code, eachData)\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                temp_returnList.append(tmp_txt)\n",
    "                if \"VARI\" in eachLine:\n",
    "                    \n",
    "                    tmp_var_list = generate_variable([eachData],True)\n",
    "                    tmp_list = generate_partial_data_VARI(\"VARI\", tmp_var_list, temp_returnList)\n",
    "                    returnList = returnList + tmp_list\n",
    "                    \n",
    "                    #print (\"EachData : \"+eachData)\n",
    "                    #print (\"tmp_var_list: {}\".format(tmp_var_list))\n",
    "                    #print(\"returnList: {}\".format(returnList))\n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "                else:\n",
    "                    returnList = returnList + temp_returnList          \n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "\n",
    "def generate_partial_data_VARI(txt_code, datafill_list, input_list):      \n",
    "    returnList = []\n",
    "    for eachLine in input_list:\n",
    "        if txt_code in eachLine:\n",
    "            count=0\n",
    "            for eachData in datafill_list: \n",
    "                tmp_txt = eachLine\n",
    "                while (txt_code in tmp_txt):\n",
    "                    if count >= len(datafill_list):\n",
    "                        count=0\n",
    "                        #print(\"Length :\",len(datafill_list))\n",
    "                    tmp_txt = tmp_txt.replace(txt_code, datafill_list[count],1)\n",
    "                    count+=1\n",
    "                #next(datafill_list,\"var1\") \n",
    "                #print(\"tmp_txt: {}\".format(tmp_txt))\n",
    "                tmp_txt = tmp_txt.replace(\"  \",\" \")\n",
    "                tmp_txt = tmp_txt.lstrip()\n",
    "                returnList.append(tmp_txt)\n",
    "        else:\n",
    "            returnList.append(eachLine)\n",
    "    return (returnList)\n",
    "\n",
    "#temp_gen_data1=[\"CMD_DECL SCOPE_OFVAR DTYPE\"]\n",
    "#print(my_func (\"DTYPE\", dataType, temp_gen_data1))\n",
    "\n",
    "\n",
    "#Substitute and create dataset\n",
    "import random\n",
    "def generate_training_data(input_txt):\n",
    "    generated_data = []\n",
    "    \n",
    "    for eachLine in input_txt:\n",
    "        print (\"eachLine: \" + eachLine)\n",
    "        \n",
    "        temp_gen_data1=[eachLine]\n",
    "        temp_gen_data2= generate_partial_data_DTYPE (\"DTYPE_NUM\", dataType_number, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data_DTYPE (\"DTYPE_CHAR\", dataType_char, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data_DTYPE (\"DTYPE_ARRAY\", dataType_list, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"DTYPE_BOOL\", dataType_bool, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"DTYPE_CLASS\", dataType_class, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data_DTYPE (\"DTYPE_VOID\", dataType_void, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        #temp_gen_data2 = generate_partial_data (\"DTYPE\", dataType, temp_gen_data1)\n",
    "        temp_gen_data2 = generate_partial_data_DTYPE (\"DTYPE\", dataType, temp_gen_data1)\n",
    "        \n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"SCOPE_OFVAR\", scope_of_variable, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"OPER\", operators, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"CMD_DECL\", command_declare, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"CMD_PRNT\", command_print, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"CMD_INPUT\", command_input, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"SPL_CLASS\", special_class, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"SPL_FUNC\", special_function, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        temp_gen_data2 = generate_partial_data (\"FUNC_PROG_START\", func_prog_start, temp_gen_data1)\n",
    "        \n",
    "        temp_gen_data1=[]\n",
    "        temp_gen_data1= generate_partial_data (\"FUNC_PROG\", func_prog, temp_gen_data2)\n",
    "        \n",
    "        temp_gen_data2=[]\n",
    "        variable_list = generate_variable (dataType, append_extra=True)\n",
    "        temp_gen_data2 = generate_partial_data_VARI(\"VARI\", variable_list, temp_gen_data1)\n",
    "        \n",
    "        generated_data = generated_data + temp_gen_data2\n",
    "    \n",
    "    temp_gen_data1=[]\n",
    "    for eachLine in generated_data:\n",
    "        if \"RAND_NUM\" in eachLine:\n",
    "            random_number = random.randint(0,100)\n",
    "            tmp_txt = eachLine.replace(\"RAND_NUM\", str(random_number))\n",
    "            temp_gen_data1.append(tmp_txt)\n",
    "        else:\n",
    "            temp_gen_data1.append(eachLine)\n",
    "    return (temp_gen_data1)\n",
    "   \n",
    "    \n",
    "#training_data = generate_training_data()\n",
    "\n",
    "#print (\"Length: \" + str( len(training_data) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eachLine: DTYPE\n",
      "eachLine: SCOPE_OFVAR\n",
      "eachLine: OPER\n",
      "eachLine: CMD_DECL\n",
      "eachLine: CMD_PRNT\n",
      "eachLine: CMD_INPUT\n",
      "eachLine: SPL_CLASS\n",
      "eachLine: SPL_FUNC\n",
      "eachLine: FUNC_PROG\n",
      "eachLine: VARI\n",
      "eachLine: RAND_NUM\n",
      "Length: 225\n",
      "['integer', 'integers', 'int', 'ints', 'float', 'floats', 'number', 'numbers', 'num', 'nums', 'variable', 'variables', 'var', 'vars', 'double', 'doubles', 'character', 'characters', 'char', 'chars', 'string', 'strings', 'str', 'strs', 'text', 'texts', 'txt', 'txts', 'array', 'arrays', 'list', 'lists', 'vector', 'vectors', 'boolean', 'booleans', 'bool', 'bools', 'class', 'classes', 'void', 'voids', 'global', 'local', '', 'addition', 'add', '+', 'subtraction', 'sub', '-', 'minus', 'multiply', 'mul', '*', 'divide', 'div', '/', 'modulo', 'modulus', 'mod', '%', 'create', 'creates', 'define', 'defines', 'declare', 'declares', 'set', 'sets', 'write', 'writes', 'print', 'prints', 'display', 'displays', 'show', 'shows', 'get', 'gets', 'input', 'inputs', 'log', 'printer', 'file', 'database', 'save', 'function', 'func', 'program', 'programme', 'prog', 'prg', 'integer0', 'integer_0', 'integer1', 'integer_1', 'integer2', 'integer_2', 'int0', 'int_0', 'int1', 'int_1', 'int2', 'int_2', 'float0', 'float_0', 'float1', 'float_1', 'float2', 'float_2', 'number0', 'number_0', 'number1', 'number_1', 'number2', 'number_2', 'num0', 'num_0', 'num1', 'num_1', 'num2', 'num_2', 'variable0', 'variable_0', 'variable1', 'variable_1', 'variable2', 'variable_2', 'var0', 'var_0', 'var1', 'var_1', 'var2', 'var_2', 'double0', 'double_0', 'double1', 'double_1', 'double2', 'double_2', 'character0', 'character_0', 'character1', 'character_1', 'character2', 'character_2', 'char0', 'char_0', 'char1', 'char_1', 'char2', 'char_2', 'string0', 'string_0', 'string1', 'string_1', 'string2', 'string_2', 'str0', 'str_0', 'str1', 'str_1', 'str2', 'str_2', 'text0', 'text_0', 'text1', 'text_1', 'text2', 'text_2', 'txt0', 'txt_0', 'txt1', 'txt_1', 'txt2', 'txt_2', 'array0', 'array_0', 'array1', 'array_1', 'array2', 'array_2', 'list0', 'list_0', 'list1', 'list_1', 'list2', 'list_2', 'vector0', 'vector_0', 'vector1', 'vector_1', 'vector2', 'vector_2', 'boolean0', 'boolean_0', 'boolean1', 'boolean_1', 'boolean2', 'boolean_2', 'bool0', 'bool_0', 'bool1', 'bool_1', 'bool2', 'bool_2', 'class0', 'class_0', 'class1', 'class_1', 'class2', 'class_2', 'void0', 'void_0', 'void1', 'void_1', 'void2', 'void_2', 'i', 'j', 'k', 'b', 'c', '80']\n"
     ]
    }
   ],
   "source": [
    "#Create Output File - #Prepare Data\n",
    "\n",
    "#DTYPE_NUM    : dataType_number\n",
    "#DTYPE_CHAR   : dataType_char\n",
    "#DTYPE_ARRAY  :dataType_list\n",
    "#DTYPE_BOOL   :dataType_bool\n",
    "#DTYPE_CLASS  :dataType_class\n",
    "#DTYPE_VOID   :dataType_void\n",
    "#DTYPE : dataType\n",
    "\n",
    "#SCOPE_OFVAR : scope_of_variable\n",
    "\n",
    "#OPER : operators\n",
    "#\n",
    "#CMD_DECL  : command_declare\n",
    "#CMD_PRNT  : command_print\n",
    "#CMD_INPUT : command_input\n",
    "\n",
    "#SPL_CLASS: special_class   : [\"log\", \"printer\"]\n",
    "#SPL_FUNC: special_function : [\"save\"]\n",
    "\n",
    "#FUNC_PROG : func_prog\n",
    "#FUNC_PROG_START : func_prog_start\n",
    "\n",
    "#VARI : <Variables>\n",
    "#RAND_NUM : <Random Number>\n",
    "input_txt=[]\n",
    "\n",
    "\n",
    "input_txt=[\n",
    "        \"DTYPE\",\n",
    "        \"SCOPE_OFVAR\",\n",
    "        \"OPER\",\n",
    "        \"CMD_DECL\",\n",
    "        \"CMD_PRNT\",\n",
    "        \"CMD_INPUT\",\n",
    "        \"SPL_CLASS\",\n",
    "        \"SPL_FUNC\",\n",
    "        \"FUNC_PROG\",\n",
    "        \"VARI\",\n",
    "        \"RAND_NUM\",\n",
    "          ]\n",
    "\n",
    "training_data = generate_training_data(input_txt)\n",
    "\n",
    "print (\"Length: \" + str( len(training_data) ) )\n",
    "print (training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "#Creating Training Data\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(training_data):\n",
    "#for doc in nlp.pipe(fileData):\n",
    "    \n",
    "    #x = re.sub(\"=\", \" = \", doc)\n",
    "    #doc = re.sub(\"  \",\" \", x)\n",
    "    \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    \n",
    "    #Modified Start\n",
    "    entity_label=[]\n",
    "    entity_label = [(nlp.vocab.strings[match_id]) for match_id, start, end in matcher(doc)]\n",
    "    #if entity_label!=[]:\n",
    "    #    print (entity_label)\n",
    "    entities = [(span.start_char, span.end_char, entity_label[index]) for index,span in enumerate(spans)]\n",
    "    \n",
    "    #for eachWord in entities:\n",
    "    #    print(\"Entities: {}\".format(eachWord))\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"tags\": entities})\n",
    "    \n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    #Modified End\n",
    "    \n",
    "    '''\n",
    "    entities = [(span.start_char, span.end_char, \"INITIALIZATION\") for span in spans]\n",
    "    \n",
    "    for eachWord in entities:\n",
    "        print(\"Entities: {}\".format(eachWord))\n",
    "        \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    '''\n",
    "    \n",
    "\n",
    "#print(*TRAINING_DATA, sep=\"\\n\")\n",
    "print(len(TRAINING_DATA))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store to Data file for inspection\n",
    "with open(\"training_data_pos.txt\",\"w\") as o_file:\n",
    "    for item in TRAINING_DATA:\n",
    "        o_file.write(\"{}\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TRAINING_DATA.pop()\n",
    "#for i in range(46436):\n",
    "    #TRAINING_DATA.pop()\n",
    "#for i in range(5000):\n",
    "#    TRAINING_DATA.pop(0)\n",
    "len(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n",
      "{'tagger': 0.0}\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "import random\n",
    "\n",
    "#nlp = spacy.blank(\"en\")\n",
    "#pos_tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(pos_tagger)\n",
    "    \n",
    "nlp = spacy.blank(\"en\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "nlp.add_pipe(tagger) \n",
    "\n",
    "\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "tagger.add_label(\"OTHER\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "#if None in TRAINING_DATA:\n",
    "#    print(\"Hello there! General Error\")\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1000):\n",
    "\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "        \n",
    "        #texts, annotations = zip(batch)\n",
    "        \n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-204-2bc93c7caeb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Losses\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;31m# Allow dict of args to GoldParse, instead of GoldParse objects.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_docs_and_golds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_format_docs_and_golds\u001b[1;34m(self, docs, golds)\u001b[0m\n\u001b[0;32m    480\u001b[0m                     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE151\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munexpected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                 \u001b[0mgold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             \u001b[0mdoc_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mgold_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mgold.pyx\u001b[0m in \u001b[0;36mspacy.gold.GoldParse.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['ner', 'parser'])\n",
    "\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(3):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAINING_DATA, size=1000)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags [('Create', '\"\"', 'PUNCT'), ('program', '\"\"', 'PUNCT'), ('for', '\"\"', 'PUNCT'), ('create', '\"\"', 'PUNCT'), ('global', '\"\"', 'PUNCT'), ('integer', '\"\"', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Create program for create global integer\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'tagger': 0.0}\n",
      "Losses {'tagger': 0.0}\n",
      "Losses {'tagger': 0.0}\n"
     ]
    }
   ],
   "source": [
    "TAG_MAP = {\"noun\": {\"pos\": \"NOUN\"}, \"verb\": {\"pos\": \"VERB\"}, \"adj\": {\"pos\": \"ADJ\"}, \"adv\": {\"pos\": \"ADV\"}}\n",
    "\n",
    "#Training Loop\n",
    "import random\n",
    "\n",
    "#nlp = spacy.blank(\"en\")\n",
    "#pos_tagger = nlp.create_pipe(\"tagger\")\n",
    "#nlp.add_pipe(pos_tagger)\n",
    "\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "for tag, values in TAG_MAP.items():\n",
    "    tagger.add_label(tag, values)\n",
    "#nlp.add_pipe(tagger)\n",
    "\n",
    "tagger.add_label(\"DATATYPE\")\n",
    "tagger.add_label(\"SCOPE\")\n",
    "tagger.add_label(\"OPERATOR\")\n",
    "tagger.add_label(\"COMMAND\")\n",
    "tagger.add_label(\"FUNC_PROG\")\n",
    "tagger.add_label(\"VARI\")\n",
    "tagger.add_label(\"SPL_CLASS\")\n",
    "tagger.add_label(\"VALUE\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "'''\n",
    "# Loop for 10 iterations\n",
    "for itn in range(3):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=1000):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses)\n",
    "'''\n",
    "    \n",
    "for i in range(3):\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAINING_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-11c39fc1906d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Losses\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;31m# Allow dict of args to GoldParse, instead of GoldParse objects.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_docs_and_golds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\miniconda3\\envs\\ME-NLP\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m_format_docs_and_golds\u001b[1;34m(self, docs, golds)\u001b[0m\n\u001b[0;32m    480\u001b[0m                     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE151\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munexpected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                 \u001b[0mgold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGoldParse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             \u001b[0mdoc_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mgold_objs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mgold.pyx\u001b[0m in \u001b[0;36mspacy.gold.GoldParse.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "TAG_MAP = {\"noun\": {\"pos\": \"NOUN\"}, \"verb\": {\"pos\": \"VERB\"}, \"adj\": {\"pos\": \"ADJ\"}, \"adv\": {\"pos\": \"ADV\"}}\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    ('Afrotropical', {'tags': ['adj']}), ('Afrocentricity', {'tags': ['noun']}),\n",
    "    ('Afrocentric', {'tags': ['adj']}), ('Afrocentrism', {'tags': ['noun']}),\n",
    "    ('Anglomania', {'tags': ['noun']}), ('Anglocentric', {'tags': ['adj']}),\n",
    "    ('apraxic', {'tags': ['adj']}), ('aglycosuric', {'tags': ['adj']}),\n",
    "    ('asecretory', {'tags': ['adj']}), ('aleukaemic', {'tags': ['adj']}),\n",
    "    ('agrin', {'tags': ['adj']}), ('Eurotransplant', {'tags': ['noun']}),\n",
    "    ('Euromarket', {'tags': ['noun']}), ('Eurocentrism', {'tags': ['noun']}),\n",
    "    ('adendritic', {'tags': ['adj']}), ('asynaptic', {'tags': ['adj']}),\n",
    "    ('Asynapsis', {'tags': ['noun']}), ('ametabolic', {'tags': ['adj']})\n",
    "]\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=['ner', 'parser'])\n",
    "\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(3):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "# test the trained model\n",
    "test_text = \"I like Afrotropical apraxic blue eggs and Afrocentricity. A Eurotransplant is cool too. The agnathostomatous Euromarket and asypnapsis is even cooler. What about Eurocentrism?\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
